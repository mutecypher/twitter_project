{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing\n",
    "\n",
    "based on work by Magnus Pederson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/y2fjlm8n4752m4cv89kq3r0h0000gn/T/ipykernel_25855/1242326755.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2362\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3531\u001b[0m         \"\"\"\n\u001b[1;32m   3532\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3533\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib_inline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend_inline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure_inline_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3534\u001b[0m         \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Distributed under the terms of the BSD 3-Clause License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from matplotlib.backends.backend_agg import (  # noqa\n\u001b[1;32m      8\u001b[0m     \u001b[0mnew_figure_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import several things from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam, Ftrl, Adamax, SGD, Adadelta, Nadam, Optimizer, RMSprop, Adagrad\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,CSVLogger, ReduceLROnPlateau,TensorBoard, LambdaCallback\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.activations import relu, sigmoid ,softmax, tanh, hard_sigmoid, softsign, softplus, linear\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Input\n",
    "from tensorflow.keras.layers import Reshape, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "from skopt.utils import use_named_args\n",
    "import sklearn.model_selection as sk\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was developed using Python 3.6 (Anaconda) and package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "For Training use the twitter data set with sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = '/Volumes/Elements/GitHub/twitter-project/Data Files/twitter_sentiment_learn.csv'\n",
    "\n",
    "learning_df = pd.read_csv(file)\n",
    "print(learning_df.shape)\n",
    "print(learning_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change this if you want the files saved in another directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training- and test-sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x = learning_df['text'].to_list()\n",
    "\n",
    "\n",
    "\n",
    "learning_df[[\"bad\", \"meh\", \"good\"]] = 0\n",
    "for i in range(learning_df.shape[0]):\n",
    "    if (learning_df.loc[i,\"NEG\"] >= learning_df.loc[i,\"NEU\"]) and (learning_df.loc[i,\"NEG\"] >= learning_df.loc[i,\"POS\"]):\n",
    "        learning_df.loc[i,\"bad\"] = 1\n",
    "    elif (learning_df.loc[i,\"NEU\"] >= learning_df.loc[i,\"NEG\"]) and (learning_df.loc[i,\"NEU\"] >= learning_df.loc[i,\"POS\"]):\n",
    "        learning_df.loc[i,\"meh\"] = 1\n",
    "    else:\n",
    "        learning_df.loc[i,\"good\"] = 1\n",
    "\n",
    "y = learning_df[[\"bad\", \"meh\", \"good\"]]\n",
    "\n",
    "\n",
    "##y = tarmac\n",
    "\n",
    "x_train,x_test,y_train,y_test= sk.train_test_split(x,y,test_size=0.20, random_state = 42)\n",
    "\n",
    "# Convert to numpy arrays.\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "##print(x[1])\n",
    "\n",
    "print(\"Train-set size: \", len(x_train))\n",
    "print(\"Test-set size:  \", len(x_test))\n",
    "\n",
    "data_text = x_train + x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "data_text = x_train + x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print an example from the training-set to see that the data looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", x_train[1])\n",
    "print(\"\\n\", learning_df.head())\n",
    "print(\"\\n\", y_train[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true \"class\" is a sentiment of the movie-review. It is a value of 0.0 for a negative sentiment and 1.0 for a positive sentiment. In this case the review is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "A neural network cannot work directly on text-strings so we must convert it somehow. There are two steps in this conversion, the first step is called the \"tokenizer\" which converts words to integers and is done on the data-set before it is input to the neural network. The second step is an integrated part of the neural network itself and is called the \"embedding\"-layer, which is described further below.\n",
    "\n",
    "We may instruct the tokenizer to only use e.g. the 10000 most popular words from the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## can be played with \n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False, oov_token=None)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(data_text)\n",
    "\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer can then be \"fitted\" to the data-set. This scans through all the text and strips it from unwanted characters such as punctuation, and also converts it to lower-case characters. The tokenizer then builds a vocabulary of all unique words along with various data-structures for accessing the data.\n",
    "\n",
    "Note that we fit the tokenizer on the entire data-set so it gathers words from both the training- and test-data. This is OK as we are merely building a vocabulary and want it to be as complete as possible. The actual neural network will of course only be trained on the training-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the entire vocabulary then set `num_words=None` above, and then it will automatically be set to the vocabulary-size here. (This is because of Keras' somewhat awkward implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(num_words)\n",
    "if num_words is None:\n",
    "    num_words = len(tokenizer.word_index)\n",
    "    print(\"the num_words is \", num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then inspect the vocabulary that has been gathered by the tokenizer. This is ordered by the number of occurrences of the words in the data-set. These integer-numbers are called word indices or \"tokens\" because they uniquely identify each word in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the tokenizer to convert all texts in the training-set to lists of these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here is a text from the training-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##print(\"\\n\",learning_df.loc[160625,:])\n",
    "print(\"\\n\", x_train_tokens)\n",
    "##print(y[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text corresponds to the following list of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the texts in the test-set to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding and Truncating Data\n",
    "\n",
    "The Recurrent Neural Network can take sequences of arbitrary length as input, but in order to use a whole batch of data, the sequences need to have the same length. There are two ways of achieving this: (A) Either we ensure that all sequences in the entire data-set have the same length, or (B) we write a custom data-generator that ensures the sequences have the same length within each batch.\n",
    "\n",
    "Solution (A) is simpler but if we use the length of the longest sequence in the data-set, then we are wasting a lot of memory. This is particularly important for larger data-sets.\n",
    "\n",
    "So in order to make a compromise, we will use a sequence-length that covers most sequences in the data-set, and we will then truncate longer sequences and pad shorter sequences.\n",
    "\n",
    "First we count the number of tokens in all the sequences in the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of tokens in a sequence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.mean(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of tokens in a sequence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The max number of tokens we will allow is set to the average plus 3 standard deviations.\n",
    "\n",
    "\n",
    "# In this case, try the full amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##max_tokens = np.mean(num_tokens) + 3 * np.std(num_tokens)\n",
    "max_tokens = np.max(num_tokens)\n",
    "max_tokens = math.floor(max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers about 95% of the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When padding or truncating the sequences that have a different length, we need to determine if we want to do this padding or truncating 'pre' or 'post'. If a sequence is truncated, it means that a part of the sequence is simply thrown away. If a sequence is padded, it means that zeros are added to the sequence.\n",
    "\n",
    "So the choice of 'pre' or 'post' can be important because it determines whether we throw away the first or last part of a sequence when truncating, and it determines whether we add zeros to the beginning or end of the sequence when padding. This may confuse the Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pad = 'pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix for the test-set has the same shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_test_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we had the following sequence of tokens above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "np.array(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train_pad[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Inverse Map\n",
    "\n",
    "For some strange reason, the Keras implementation of a tokenizer does not seem to have the inverse mapping from integer-tokens back to words, which is needed to reconstruct text-strings from lists of tokens. So we make that mapping here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper-function for converting a list of tokens back to a string of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, this is the original text from the data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Recurrent Neural Network\n",
    "\n",
    "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer in the RNN is a so-called Embedding-layer which converts each integer-token into a vector of values. This is necessary because the integer-tokens may take on values between 0 and 10000 for a vocabulary of 10000 words. The RNN cannot work on values in such a wide range. The embedding-layer is trained as a part of the RNN and will learn to map words with similar semantic meanings to similar embedding-vectors, as will be shown further below.\n",
    "\n",
    "First we define the size of the embedding-vector for each integer-token. In this case we have set it to 8, so that each integer-token will be converted to a vector of length 8. The values of the embedding-vector will generally fall roughly between -1.0 and 1.0, although they may exceed these values somewhat.\n",
    "\n",
    "The size of the embedding-vector is typically selected between 100-300, but it seems to work reasonably well with small values for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "figure_of_merit = 2 * max_tokens ## was 100\n",
    "first_layer = math.floor(max_tokens/2) + 2\n",
    "second_layer = math.floor(max_tokens/3) + 2\n",
    "third_layer = math.floor(max_tokens/4) + 3\n",
    "fourth_layer = math.floor(max_tokens/5) + 3\n",
    "embedding_size = figure_of_merit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.add(Embedding(input_dim=num_words,## was num_words\n",
    "                    output_dim=embedding_size,                    \n",
    "                    input_length=max_tokens,\n",
    "                   name='layer_embedding'))\n",
    "##model.add(Flatten())\n",
    "\n",
    "##model = tf.keras.Sequential([\n",
    "##    tf.keras.layers.Embedding(input_dim = num_words, output_dim = embedding_size, input_length=max_tokens),\n",
    "##    tf.keras.layers.Flatten(),\n",
    "##    tf.keras.layers.Dense(30, activation='relu'),\n",
    "##    tf.keras.layers.Dense(60, activation='sigmoid')\n",
    "##    tf.keras.layers.Dense(15, activation='relu')\n",
    "##    tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "##])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the first Gated Recurrent Unit (GRU) to the network. This will have 16 outputs. Because we will add a second GRU after this one, we need to return sequences of data because the next GRU expects sequences as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.add(GRU(units=first_layer, \n",
    "              activation = 'tanh', ## was tanh\n",
    "              recurrent_activation = 'softmax', \n",
    "              return_sequences=True))\n",
    "##model.add(Dense(first_layer, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds the second GRU with 8 output units. This will be followed by another GRU so it must also return sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.add(GRU(units=second_layer, activation = 'tanh' , ## was tanh\n",
    "              recurrent_activation = 'softmax', \n",
    "              return_sequences=True))\n",
    "##model.add(Dense(second_layer, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds the third and final GRU with 4 output units. This will be followed by a dense-layer, so it should only give the final output of the GRU and not a whole sequence of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.add(GRU(units=third_layer, activation = 'tanh', ## was tanh\n",
    "              recurrent_activation = 'softmax', \n",
    "              return_sequences = True))\n",
    "model.add(GRU(units=fourth_layer, activation = 'tanh', ## was tanh\n",
    "              recurrent_activation = 'sigmoid', return_sequences = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a fully-connected / dense layer which computes a value between 0 and 1.0 that will be used as the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.add(Dense(3, activation='softmax')) ## was 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Adam optimizer with the given learning-rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "learning_rat = 1e-3\n",
    "optimizer = Nadam(learning_rate=learning_rat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the Keras model so it is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "             metrics=['accuracy'])\n",
    "##model.compile(loss='binary_crossentropy',\n",
    "##              optimizer=optimizer,\n",
    "##              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "callbackx = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                             patience =1,\n",
    "                                            restore_best_weights = True)\n",
    "        \n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('val_accuracy') > 0.95):\n",
    "      print(\"\\nReached 95% val_accuracy, so slowing the learning rate and keeping Nadam optimizer.\")\n",
    "      optimizer = Nadam(learning_rate = 0.4*learning_rat)\n",
    "      self.model.stop_training = False\n",
    "    if(logs.get('val_accuracy') > 0.970):\n",
    "      print(\"\\nReached 987 val_accuracy, so slowing the learning rate and keeping Nadam optimizer.\")\n",
    "      optimizer = Nadam(learning_rate = 0.1*learning_rat)\n",
    "      self.model.stop_training = False\n",
    "call_it = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Recurrent Neural Network\n",
    "\n",
    "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.15, epochs=10, batch_size= 256,\n",
    "            callbacks = [callbackx, call_it])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test-Set\n",
    "\n",
    "Now that the model has been trained we can calculate its classification accuracy on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train_pad, y_train,\n",
    "          validation_split=0.25, epochs=40, batch_size=100, verbose = 2,\n",
    "            callbacks = [call_it, callbackx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "result = model.evaluate(x_test_pad, y_test)\n",
    "print(\"and now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:.2%}\".format(result[1]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##from keras.models import models_from_json\n",
    "model_json = model.to_json()\n",
    "with open (\"model_cat.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"model_cat.h5\")\n",
    "\n",
    "model.save(\"model_cat.h5\")\n",
    "print(model_json)\n",
    "print(\"saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Mis-Classified Text\n",
    "\n",
    "In order to show an example of mis-classified text, we first calculate the predicted sentiment for the first 1000 texts in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Load the saved model and see if the results differ\n",
    "\n",
    "model_file = 'model_cat.h5'\n",
    "\n",
    "loaded_model = load_model(\n",
    "    model_file,\n",
    "    custom_objects = None,\n",
    "    compile = True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##y_pred = model.predict(x=x_test_pad[0:1000])\n",
    "##y_pred = loaded_model.predict(x=x_test_pad[0:1000])\n",
    "##y_pred = y_pred.T[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predicted numbers fall between 0.0 and 1.0. We use a cutoff / threshold and say that all values above 0.5 are taken to be 1.0 and all values below 0.5 are taken to be 0.0. This gives us a predicted \"class\" of either 0.0 or 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and use that to create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "preddy = loaded_model.predict(x_test_pad)\n",
    "\n",
    "preddy = np.array(preddy)\n",
    "\n",
    "catzy_df = pd.DataFrame(preddy, columns = ['NEG', 'NEU', 'POS'])\n",
    "\n",
    "catzy_df[[\"neg\", \"neu\", \"pos\"]] = 0\n",
    "for i in range(catzy_df.shape[0]):\n",
    "    if (catzy_df.loc[i,\"NEG\"] >= catzy_df.loc[i,\"NEU\"]) and (catzy_df.loc[i,\"NEG\"] >= catzy_df.loc[i,\"POS\"]):\n",
    "        catzy_df.loc[i,\"neg\"] = 1\n",
    "    elif (catzy_df.loc[i,\"NEU\"] >= catzy_df.loc[i,\"NEG\"]) and (catzy_df.loc[i,\"NEU\"] >= catzy_df.loc[i,\"POS\"]):\n",
    "        catzy_df.loc[i,\"neu\"] = 1\n",
    "    else:\n",
    "        catzy_df.loc[i,\"pos\"] = 1\n",
    "\n",
    "preddy = catzy_df[[\"neg\", \"neu\", \"pos\"]]\n",
    "\n",
    "\n",
    "\n",
    "print(\"The head of preddy is \\n\",preddy.head())\n",
    "##print(\"The head of y_test is \\n\",y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "\n",
    "print('Precision                                   : %.3f'%precision_score(y_test, preddy, average = 'weighted'))\n",
    "print('Recall                                      : %.3f'%recall_score(y_test, preddy, average = 'weighted'))\n",
    "print('F1-Score                                    : %.3f'%f1_score(y_test, preddy, average = 'weighted'))\n",
    "print('\\nPrecision Recall F1-Score Support Per Class : \\n',precision_recall_fscore_support(y_test, preddy, average = 'weighted'))\n",
    "print('\\nClassification Report                       : ')\n",
    "print(classification_report(y_test, preddy))\n",
    "\n",
    "\n",
    "turkey = 0*preddy['neg'] + preddy['neu'] + 2*preddy['pos']\n",
    "\n",
    "print(\"the value counts for turkey are \\n\", turkey.value_counts())\n",
    "\n",
    "grubby = np.array(y_test)\n",
    "\n",
    "y_catty = pd.DataFrame(grubby, columns = ['neg', 'neu', 'pos'])\n",
    "\n",
    "doggy = 0*y_catty['neg'] + y_catty['neu'] + 2*y_catty['pos']\n",
    "\n",
    "print(\"the value counts for doggy are \\n\", doggy.value_counts())\n",
    "\n",
    "##cm = multilabel_confusion_matrix(y_true=y_test, y_pred=preddy)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(doggy, turkey)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['Negative','Neutral','Positive'], \n",
    "                     columns = ['Negative','Neutral','Positive'])\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "sns.heatmap(cm_df, annot=True)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data\n",
    "\n",
    "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do Amazon with the category stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "amzn_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_json.csv'\n",
    "\n",
    "amzn_df =  pd.read_csv(amzn_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "am_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'AMZN', 'Amazon', 'dupe']\n",
    "amzn_df.columns = am_columns\n",
    "\n",
    "amzn_text = amzn_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "amzn_x = amzn_text.to_list()\n",
    "\n",
    "bob = amzn_x[0:7]\n",
    "\n",
    "print(type(bob))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(amzn_x)\n",
    "\n",
    "amzn_train_tokens = tokenizer.texts_to_sequences(amzn_x)\n",
    "\n",
    "amzn_pad = pad_sequences(amzn_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "amzn_sent = loaded_model.predict(x=amzn_pad)\n",
    "\n",
    "print(amzn_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(amzn_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "amzn_df_nn = amzn_df.join(predictions)\n",
    "amzn_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol','Name', 'dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "amzn_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of Amazon_df_nn is ', amzn_df_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "KMI_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv'\n",
    "\n",
    "KMI_df =  pd.read_csv(KMI_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "KMI_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'KMI', 'Kinder_Morgan', 'dupe']\n",
    "KMI_df.columns = KMI_columns\n",
    "\n",
    "KMI_text = KMI_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "KMI_x = KMI_text.to_list()\n",
    "\n",
    "print(type(KMI_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(KMI_x)\n",
    "\n",
    "KMI_train_tokens = tokenizer.texts_to_sequences(KMI_x)\n",
    "\n",
    "KMI_pad = pad_sequences(KMI_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "KMI_sent = loaded_model.predict(x=KMI_pad)\n",
    "\n",
    "print(KMI_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(KMI_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "KMI_df_nn = KMI_df.join(predictions)\n",
    "KMI_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "KMI_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of KMI _df_nn is ', KMI_df_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrowdSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "CRWD_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_df_json.csv'\n",
    "\n",
    "CRWD_df =  pd.read_csv(CRWD_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "CRWD_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'CRWD', 'CrowdSource', 'dupe']\n",
    "CRWD_df.columns = CRWD_columns\n",
    "\n",
    "CRWD_text = CRWD_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "CRWD_x = CRWD_text.to_list()\n",
    "\n",
    "print(type(CRWD_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(CRWD_x)\n",
    "\n",
    "CRWD_train_tokens = tokenizer.texts_to_sequences(CRWD_x)\n",
    "\n",
    "CRWD_pad = pad_sequences(CRWD_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "CRWD_sent = loaded_model.predict(x=CRWD_pad)\n",
    "\n",
    "print(CRWD_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(CRWD_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "CRWD_df_nn = CRWD_df.join(predictions)\n",
    "CRWD_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "CRWD_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of CRWD_df_nn is ', CRWD_df_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Appian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "APPN_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_df_json.csv'\n",
    "\n",
    "APPN_df =  pd.read_csv(APPN_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "APPN_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'APPN', 'Appian', 'dupe']\n",
    "APPN_df.columns =APPN_columns\n",
    "\n",
    "APPN_text = APPN_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "APPN_x = APPN_text.to_list()\n",
    "\n",
    "print(type(APPN_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(APPN_x)\n",
    "\n",
    "APPN_train_tokens = tokenizer.texts_to_sequences(APPN_x)\n",
    "\n",
    "APPN_pad = pad_sequences(APPN_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "APPN_sent = loaded_model.predict(x=APPN_pad)\n",
    "\n",
    "print(APPN_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(APPN_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "APPN_df_nn = APPN_df.join(predictions)\n",
    "APPN_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "APPN_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of APPN_df_nn is ', APPN_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "INSP_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_df_json.csv'\n",
    "\n",
    "INSP_df =  pd.read_csv(INSP_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "INSP_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'INSP', 'Inspire_Systems', 'dupe']\n",
    "INSP_df.columns = INSP_columns\n",
    "\n",
    "INSP_text = INSP_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "INSP_x = INSP_text.to_list()\n",
    "\n",
    "print(type(INSP_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(INSP_x)\n",
    "\n",
    "INSP_train_tokens = tokenizer.texts_to_sequences(INSP_x)\n",
    "\n",
    "INSP_pad = pad_sequences(INSP_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "INSP_sent = loaded_model.predict(x=INSP_pad)\n",
    "\n",
    "print(INSP_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(INSP_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "INSP_df_nn = INSP_df.join(predictions)\n",
    "INSP_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "INSP_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of INSP_df_nn is ', INSP_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanofy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "SNY_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_df_json.csv'\n",
    "\n",
    "SNY_df =  pd.read_csv(SNY_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "SNY_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'SNY', 'Sanofy', 'dupe']\n",
    "SNY_df.columns = SNY_columns\n",
    "\n",
    "SNY_text = SNY_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "SNY_x = SNY_text.to_list()\n",
    "\n",
    "print(type(SNY_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(SNY_x)\n",
    "\n",
    "SNY_train_tokens = tokenizer.texts_to_sequences(SNY_x)\n",
    "\n",
    "SNY_pad = pad_sequences(SNY_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "SNY_sent = loaded_model.predict(x=SNY_pad)\n",
    "\n",
    "print(SNY_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(SNY_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "SNY_df_nn = SNY_df.join(predictions)\n",
    "SNY_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "SNY_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of SNY_df_nn is ', SNY_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "EVBG_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_df_json.csv'\n",
    "\n",
    "EVBG_df =  pd.read_csv(EVBG_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "EVBG_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'EVBG', 'Everbridge', 'dupe']\n",
    "EVBG_df.columns = EVBG_columns\n",
    "\n",
    "EVBG_text = EVBG_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "EVBG_x = EVBG_text.to_list()\n",
    "\n",
    "print(type(EVBG_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(EVBG_x)\n",
    "\n",
    "EVBG_train_tokens = tokenizer.texts_to_sequences(EVBG_x)\n",
    "\n",
    "EVBG_pad = pad_sequences(EVBG_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "EVBG_sent = loaded_model.predict(x=EVBG_pad)\n",
    "\n",
    "print(EVBG_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(EVBG_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "EVBG_df_nn = EVBG_df.join(predictions)\n",
    "EVBG_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "EVBG_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of EVBG_df_nn is ', EVBG_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "XOM_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/XOM_df_json.csv'\n",
    "\n",
    "XOM_df =  pd.read_csv(XOM_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "XOM_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'XOM', 'Exxon', 'dupe']\n",
    "XOM_df.columns = XOM_columns\n",
    "\n",
    "XOM_text = XOM_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "XOM_x = XOM_text.to_list()\n",
    "\n",
    "print(type(XOM_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(XOM_x)\n",
    "\n",
    "XOM_train_tokens = tokenizer.texts_to_sequences(XOM_x)\n",
    "\n",
    "XOM_pad = pad_sequences(XOM_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "XOM_sent = loaded_model.predict(x=XOM_pad)\n",
    "\n",
    "print(XOM_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(XOM_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "XOM_df_nn = XOM_df.join(predictions)\n",
    "XOM_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "XOM_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/XOM_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of XOM_df_nn is ', XOM_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Finally, GMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "GMED_file = '/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_df_json.csv'\n",
    "\n",
    "GMED_df =  pd.read_csv(GMED_file, header=0, index_col=0, parse_dates=True)\n",
    "\n",
    "GMED_columns = ['numbers', 'created_at','full_text', 'retweet_count', 'user_id', \n",
    "           'user.favourites_count', 'user.followers_count', 'GMED' , 'Global', 'dupe']\n",
    "GMED_df.columns = GMED_columns\n",
    "\n",
    "GMED_text = GMED_df['full_text'].astype(str)\n",
    "\n",
    "\n",
    "GMED_x = GMED_text.to_list()\n",
    "\n",
    "print(type(GMED_x))\n",
    "\n",
    "num_words = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "\n",
    "##%%time\n",
    "tokenizer.fit_on_texts(GMED_x)\n",
    "\n",
    "GMED_train_tokens = tokenizer.texts_to_sequences(GMED_x)\n",
    "\n",
    "GMED_pad = pad_sequences(GMED_train_tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "\n",
    "GMED_sent = loaded_model.predict(x=GMED_pad)\n",
    "\n",
    "print(GMED_sent[1:3])\n",
    "\n",
    "predictions = pd.DataFrame(GMED_sent, columns  = ['neg', 'neu', 'pos'])\n",
    "\n",
    "GMED_df_nn = GMED_df.join(predictions)\n",
    "GMED_df_nn.columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "\n",
    "GMED_df_nn.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_nn_scored_cat.csv', header = True)\n",
    "\n",
    "print('the shape of GMED_df_nn is ', GMED_df_nn.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "text9 = 'This was an awesome movie. I watch it twice my time watching this beautiful movie if I have known it was this good'\n",
    "text10 = 'One of the worst movies of all time. I cannot believe I wasted two hours of my life for this movie'\n",
    "text11 = 'At no point can I say I loved this movie.'\n",
    "text12 = 'No complaint about this movie could be justified, just pure perfection.'\n",
    "\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, text11, text12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first convert these texts to arrays of integer-tokens because that is needed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To input texts with different lengths into the model, we also need to pad and truncate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
    "                           padding=pad, truncating=pad)\n",
    "tokens_pad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the trained model to predict the sentiment for these texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "## Note, using the loaded prediciton model.\n",
    "\n",
    "loaded_model.predict(tokens_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
    "\n",
    "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
    "\n",
    "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
    "\n",
    "First we need to get the embedding-layer from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "layer_embedding = model.get_layer('layer_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then get the weights used for the mapping done by the embedding-layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "weights_embedding = layer_embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "weights_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "token_good = tokenizer.word_index['good']\n",
    "token_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also get the integer-token for the word 'great'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "token_great = tokenizer.word_index['great']\n",
    "token_great"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
    "\n",
    "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "weights_embedding[token_good]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "weights_embedding[token_great]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##token_bad = tokenizer.word_index['bad']\n",
    "##token_horrible = tokenizer.word_index['horrible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##weights_embedding[token_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##weights_embedding[token_horrible]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted Words\n",
    "\n",
    "We can also sort all the words in the vocabulary according to their \"similarity\" in the embedding-space. We want to see if words that have similar embedding-vectors also have similar meanings.\n",
    "\n",
    "Similarity of embedding-vectors can be measured by different metrics, e.g. Euclidean distance or cosine distance.\n",
    "\n",
    "We have a helper-function for calculating these distances and printing the words in sorted order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def print_sorted_words(word, metric='cosine'):\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = tokenizer.word_index[word]\n",
    "\n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "\n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "    \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index\n",
    "                    if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##print_sorted_words('great', metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "##print_sorted_words('worst', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"\\n This finished at \", now)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial showed the basic methods for doing Natural Language Processing (NLP) using a Recurrent Neural Network with integer-tokens and an embedding layer. This was used to do sentiment analysis of movie reviews from IMDB. It works reasonably well if the hyper-parameters are chosen properly. But it is important to understand that this is not human-like comprehension of text. The system does not have any real understanding of the text. It is just a clever way of doing pattern-recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n",
    "\n",
    "You may want to backup this Notebook before making any changes.\n",
    "\n",
    "* Run more training-epochs. Does it improve performance?\n",
    "* If your model overfits the training-data, try using dropout-layers and dropout inside the GRU.\n",
    "* Increase or decrease the number of words in the vocabulary. This is done when the `Tokenizer` is initialized. Does it affect performance?\n",
    "* Increase the size of the embedding-vectors to e.g. 200. Does it affect performance?\n",
    "* Try varying all the different hyper-parameters for the Recurrent Neural Network.\n",
    "* Use Bayesian Optimization from Tutorial #19 to find the best choice of hyper-parameters.\n",
    "* Use 'post' for padding and truncating in `pad_sequences()`. Does it affect the performance?\n",
    "* Use individual characters instead of tokenized words as the vocabulary. You can then use one-hot encoded vectors for each character instead of using the embedding-layer.\n",
    "* Use `model.fit_generator()` instead of `model.fit()` and make your own data-generator, which creates a batch of data using a random subset of `x_train_tokens`. The sequences must be padded so they all match the length of the longest sequence.\n",
    "* Explain to a friend how the program works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.0 ('TFlow': venv)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/Volumes/Elements/GitHub/twitter-project/twitter_project/TFlow/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.12 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a968e1667413feb48946be4a508b5337cf88b175eae178ab54c7be153155f19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
