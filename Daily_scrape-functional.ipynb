{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Twitter Scape as of September 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version is  1.4.1\n",
      "3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "2022-06-18 13:22:25.741865\n",
      "202205191322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Revised with new tweepy api in October 22, 2021\n",
    "\n",
    "\n",
    "@author: mutecypher\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(\"pandas version is \", pd.__version__)\n",
    "import tweepy \n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "import sys\n",
    "##from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "my_bearer_token = 'AAAAAAAAAAAAAAAAAAAAADigdwEAAAAAyGEJIbrn1BPYIMXFEiICWFaB1%2F0%3DW2q0OE9Bf0w1qZGvRwwiwwE6rreDjJP3MaIn1abqaMTuxFFOSE'\n",
    "\n",
    "API_key = 'OEnnzk5ro76PdhQHTJVxa3qTv'\n",
    "##API_key = 'qchdahMH5QJp0xd1dPQPTdxAJ'\n",
    "API_SECRET_KEY = 'sj4lfiejDGosykNA62YaHqBP1J9RyESbMqKboxQp9sW51gJylC'\n",
    "\n",
    "client = tweepy.Client(bearer_token = my_bearer_token)\n",
    "##api = tweepy.API(client)\n",
    "\n",
    "items_to_get = 100\n",
    "\n",
    "whats_today = datetime.datetime.now()\n",
    "##whats_today = whats_today.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "\n",
    "\n",
    "minus_one = datetime.datetime.now() - datetime.timedelta(days = 1)\n",
    "##minus_one = minus_one.strftime(\"%Y%m%d%H%M\")\n",
    "minus_two = datetime.datetime.now() - datetime.timedelta(days = 2)\n",
    "minus_two = minus_two.strftime(\"%Y%m%d%H%M\")\n",
    "##minus_two = datetime.strptime(minus_two, \"%Y%M%d%H%m\")\n",
    "\n",
    "minus_thirty = datetime.datetime.now() - datetime.timedelta(days = 30)\n",
    "\n",
    "minus_thirty = minus_thirty.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "##minus_thirty = datetime.datetime.strptime(minus_thirty, \"%Y%M%d%H%m\")\n",
    "\n",
    "\n",
    "print(sys.version)\n",
    "print(whats_today)\n",
    "print(minus_thirty)\n",
    "##search_words = 'AMZN'\n",
    "\n",
    "\n",
    "##tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "  ##                         maxResults = 11)\n",
    "#print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "#Define a function that will take our search query, a limit of 1000 tweets by default, default to english language\n",
    "#and allow us to pass a list of words to remove from the string\n",
    "def tweetSearch(query, limit, language , remove = []):\n",
    "    \n",
    "    #Create a blank variable\n",
    "    text = \"\"\n",
    "    \n",
    "    #Iterate through Twitter using Tweepy to find our query in our language, with our defined limit\n",
    "    #For every tweet that has our query, add it to our text holder in lower case\n",
    "    for tweet in api.search_tweets(q = query, lang = language, count = limit):\n",
    "        text += tweet.text.lower()\n",
    "    \n",
    "    #Twitter has lots of links, we need to remove the common parts of links to clean our data\n",
    "    #Firstly, create a list of terms that we want to remove. This contains https & co, alongside any words in our remove list\n",
    "    removeWords = [\"https\",\"co\"]\n",
    "    removeWords += remove\n",
    "    \n",
    "    #For each word in our removeWords list, replace it with nothing in our main text - deleting it\n",
    "    for word in removeWords:\n",
    "        text = text.replace(word, \"\")\n",
    "    \n",
    "    #return our clean text\n",
    "    return text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "#Generate our text with our new function\n",
    "#Remove all mentions of the name itself, as this will obviously be the most common!\n",
    "\n",
    "\n",
    "Mkhitaryan = tweetSearch(\"Mkhitaryan\", items_to_get, \"en\", remove = [])\n",
    "\n",
    "    \n",
    "#Create the wordcloud with the text created above\n",
    "##print(Mkhitaryan)\n",
    "\n",
    "#Create the wordcloud with the text created above\n",
    "wordcloud = WordCloud().generate(Mkhitaryan)\n",
    "\n",
    "#Plot the text with the lines below\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "## let's see if we can get them all\n",
    "\n",
    "def tweet_all_Search(dev, query):\n",
    "    \n",
    "    #Create a blank variable\n",
    "    text = \"\"\n",
    "    \n",
    "    #Iterate through Twitter using Tweepy to find our query in our language, with our defined limit\n",
    "    #For every tweet that has our query, add it to our text holder in lower case\n",
    "    for tweet in api.search_full_archive(dev, query):\n",
    "        text += tweet.text.lower()\n",
    "    \n",
    "    #Twitter has lots of links, we need to remove the common parts of links to clean our data\n",
    "    #Firstly, create a list of terms that we want to remove. This contains https & co, alongside any words in our remove list\n",
    "    removeWords = [\"https\",\"co\"]\n",
    "    ##removeWords += remove\n",
    "    \n",
    "    #For each word in our removeWords list, replace it with nothing in our main text - deleting it\n",
    "    for word in removeWords:\n",
    "        text = text.replace(word, \"\")\n",
    "    \n",
    "    #return our clean text\n",
    "    return text\n",
    "\n",
    "\n",
    "##dev = 'Testingamzn'\n",
    "##query = \"AMZN\"\n",
    "##all_amzn_df = tweet_all_Search(dev, query)\n",
    "\n",
    "##all_amzn = pd.DataFrame(all_amzn_df)\n",
    "\n",
    "##print(all_amzn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  created_at                                               text source\n",
      "0       None  RT @honnoinosisi555: 「よく差別の問題は、「お互いに優しくなろう」という...   None\n",
      "1       None  遊戯王OCG デュエルモンスターズ デュエリストカードプロテクター ブルー\\n#Amazon...   None\n",
      "2       None  RT @EarthDesires: Anne Rice was my true inspir...   None\n",
      "3       None  @_gwaine RATEL Set de Peinture Aquarelle, Comp...   None\n",
      "4       None  Amazon 気になる商品をご紹介★\\n\\nApple MacBook Air Mid 20...   None\n",
      "the AMZN_rob columns are  Index(['created_at', 'text', 'source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Get the Amazon tweets\n",
    "\n",
    "\n",
    "search_words = 'AMZN'\n",
    "\n",
    "\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "tweets = client.search_recent_tweets( query = search_words, start_time = minus_one, end_time = whats_today,\n",
    "                            max_results = items_to_get) \n",
    "\n",
    "first_one = tweets.data[0]\n",
    "dict(first_one)\n",
    "##print(first_one)\n",
    "\n",
    "tweets_AMZN = []\n",
    "\n",
    "for tweet in tweets.data:\n",
    "    tweet_info = {\n",
    "        'created_at': tweet.created_at,\n",
    "        'text': tweet.text,\n",
    "        'source': tweet.source\n",
    "        }\n",
    "    tweets_AMZN.append(tweet_info)\n",
    "##print(tweets_AMZN)\n",
    "\n",
    "search_words = 'Amazon'\n",
    "\n",
    "\n",
    "tweets = client.search_recent_tweets( query = search_words, start_time = minus_one, end_time = whats_today,\n",
    "                            max_results = items_to_get)\n",
    "for tweet in tweets.data:\n",
    "    tweet_info = {\n",
    "        'created_at': tweet.created_at,\n",
    "        'text': tweet.text,\n",
    "        'source': tweet.source\n",
    "        }\n",
    "    tweets_AMZN.append(tweet_info)\n",
    "    \n",
    "Amazon_df = pd.DataFrame(tweets_AMZN)\n",
    "print(Amazon_df.head())\n",
    "\n",
    "\n",
    "Amazon_df.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_api2.csv', header = None)\n",
    "\n",
    "print(\"the AMZN_rob columns are \", Amazon_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "bob1 = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_json.csv') \n",
    "columns = ['rowz', 'not_sure','created_at','full_text', 'retweet_count', 'user_id',\n",
    "                                'user.favourites_count', 'user.followers_count', 'AMZN',\n",
    "                               'Amazon', 'dupe']\n",
    "bob1.columns = columns\n",
    "\n",
    "##bob1.drop(columns=bob1.columns[0], \n",
    "##        axis=1, \n",
    "##        inplace=True)\n",
    "print(\"first try shape is \\n\", bob1.shape)\n",
    "print(\"first try head is \\n\", bob1.head())\n",
    "print(\"first try tail is \\n\", bob1.tail())\n",
    "bob1.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_json.csv', header = False)\n",
    "bob2 = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_json.csv', header = None)\n",
    "bob2.drop(columns=bob2.columns[0], \n",
    "        axis=1, \n",
    "       inplace=True)\n",
    "bob2.columns = columns\n",
    "print(\"second shape is \\n\", bob2.shape) \n",
    "print(\"second try head is \\n\", bob2.head())\n",
    "print(\"second try tail is \\n\", bob2.tail())\n",
    "\n",
    "##bob2.compare(bob1, align_axis = 0,  keep_equal = False, keep_shape = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinder Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\n453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, you’ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=0'>1</a>\u001b[0m search_words \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mKinder Morgan\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39m##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=4'>5</a>\u001b[0m tweets \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49msearch_30_day(label \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mThirtyDaze\u001b[39;49m\u001b[39m'\u001b[39;49m, query \u001b[39m=\u001b[39;49m search_words, fromDate \u001b[39m=\u001b[39;49m minus_one, toDate \u001b[39m=\u001b[39;49m whats_today,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=5'>6</a>\u001b[0m                            maxResults \u001b[39m=\u001b[39;49m items_to_get) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=6'>7</a>\u001b[0m tweets_KMI \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Volumes/Elements/GitHub/twitter-project/twitter-project/Daily_scrape-functional.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m tweet \u001b[39min\u001b[39;00m tweets:\n",
      "File \u001b[0;32m/Volumes/Elements/GitHub/twitter-project/twitter-project/TFlow/lib/python3.9/site-packages/tweepy/api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(method)\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Volumes/Elements/GitHub/twitter-project/twitter-project/TFlow/lib/python3.9/site-packages/tweepy/api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mpayload_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m payload_list\n\u001b[1;32m     45\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mpayload_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m payload_type\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Volumes/Elements/GitHub/twitter-project/twitter-project/TFlow/lib/python3.9/site-packages/tweepy/api.py:363\u001b[0m, in \u001b[0;36mAPI.search_30_day\u001b[0;34m(self, label, query, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39m@pagination\u001b[39m(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    287\u001b[0m \u001b[39m@payload\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlist\u001b[39m\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearch_30_day\u001b[39m(\u001b[39mself\u001b[39m, label, query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    289\u001b[0m     \u001b[39m\"\"\"search_30_day(label, query, *, tag, fromDate, toDate, maxResults, \\\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m                     next)\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m    https://developer.twitter.com/en/docs/twitter-api/premium/search-api/api-reference/premium-search\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    364\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtweets/search/30day/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlabel\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m, endpoint_parameters\u001b[39m=\u001b[39;49m(\n\u001b[1;32m    365\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtag\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfromDate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtoDate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mmaxResults\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mnext\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m    366\u001b[0m         ), query\u001b[39m=\u001b[39;49mquery, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    367\u001b[0m     )\n",
      "File \u001b[0;32m/Volumes/Elements/GitHub/twitter-project/twitter-project/TFlow/lib/python3.9/site-packages/tweepy/api.py:259\u001b[0m, in \u001b[0;36mAPI.request\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[39mraise\u001b[39;00m Unauthorized(resp)\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m403\u001b[39m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mraise\u001b[39;00m Forbidden(resp)\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m404\u001b[39m:\n\u001b[1;32m    261\u001b[0m     \u001b[39mraise\u001b[39;00m NotFound(resp)\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Forbidden\n453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, you’ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve"
     ]
    }
   ],
   "source": [
    "\n",
    "search_words = 'Kinder Morgan'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get) \n",
    "tweets_KMI = []\n",
    "for tweet in tweets:\n",
    "    tweets_KMI.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'KMI'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get) \n",
    "    \n",
    "for tweet in tweets:\n",
    "    tweets_KMI.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "KMI_df = pd.DataFrame(tweets_KMI, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(KMI_df)):\n",
    "    KMI_df.loc[i,'KMI'] = 'KMI' in KMI_df.loc[i,'full_text']\n",
    "    KMI_df.loc[i,'Kinder_Morgan'] = 'Kinder Morgan' in KMI_df.loc[i,'full_text']\n",
    "    \n",
    "KMI_df['dupe']= KMI_df['full_text'].duplicated(keep = 'last')    \n",
    "\n",
    "\n",
    "print(\"KMI_df just read shape is \", KMI_df.shape)\n",
    "\n",
    "\n",
    "KMI_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'KMI', 'Kinder_Morgan', 'dupe']\n",
    "\n",
    "KMI_bob.columns = columns\n",
    "\n",
    "\n",
    "KMI_bob = KMI_bob.drop(['rowz'], axis = 1)\n",
    "\n",
    "\n",
    "KMI_rob = KMI_bob.append(KMI_df, ignore_index = True)\n",
    "print(\"KMI_rob shape is \\n\", KMI_rob.shape)\n",
    "\n",
    "\n",
    "KMI_rob = KMI_rob[(KMI_rob.KMI == True) | (KMI_rob.Kinder_Morgan == True)]\n",
    "KMI_rob['dupe']= KMI_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "print(\"KMI_rob shape after dupes and either KMI or Kinder Morgan is \\n\", KMI_rob.shape)\n",
    "##print(\"KMI_rob  head is \\n\", KMI_rob.head())\n",
    "##print(\"Bob head is \\n\", rob.head(1))\n",
    "KMI_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv', header = None)\n",
    "\n",
    "print(\"the KMI_rob columns are \", KMI_rob.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping\n"
     ]
    }
   ],
   "source": [
    "%%script echo skipping\n",
    "\n",
    "bob1 = pd.read_excel('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.xlsx') \n",
    "columns = ['rowz','created_at','full_text', 'retweet_count', 'user_id',\n",
    "                                'user.favourites_count', 'user.followers_count', 'KMI',\n",
    "                               'Kinder_Morgan', 'dupe']\n",
    "bob1.columns = columns\n",
    "\n",
    "##bob1.drop(columns=bob1.columns[0], \n",
    "##        axis=1, \n",
    "##        inplace=True)\n",
    "print(\"first try shape is \\n\", bob1.shape)\n",
    "print(\"first try head is \\n\", bob1.head())\n",
    "print(\"first try tail is \\n\", bob1.tail())\n",
    "bob1.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv', header = False)\n",
    "bob2 = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv', header = None)\n",
    "bob2.drop(columns=bob2.columns[0], \n",
    "        axis=1, \n",
    "       inplace=True)\n",
    "bob2.columns = columns\n",
    "print(\"second shape is \\n\", bob2.shape) \n",
    "print(\"second try head is \\n\", bob2.head())\n",
    "print(\"second try tail is \\n\", bob2.tail())\n",
    "\n",
    "##bob2.compare(bob1, align_axis = 0,  keep_equal = False, keep_shape = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVBG_df just read shape is  (33, 9)\n",
      "EVBG_bob shape is \n",
      " (137935, 10)\n",
      "EVBG_rob shape is \n",
      " (137968, 10)\n",
      "the EVBG_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'EVBG', 'Everbridge',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "search_words = 'Everbridge'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_EVBG = []\n",
    "for tweet in tweets:\n",
    "    tweets_EVBG.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'EVBG'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_EVBG.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "     \n",
    "    \n",
    "EVBG_df = pd.DataFrame(tweets_EVBG, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(EVBG_df)):\n",
    "    EVBG_df.loc[i,'EVBG'] = 'EVBG' in EVBG_df.loc[i,'full_text']\n",
    "    EVBG_df.loc[i,'Everbridge'] = 'Everbridge' in EVBG_df.loc[i,'full_text']\n",
    "    \n",
    "    \n",
    "colnames_EVBG = ['number', 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'EVBG', 'Everbridge']\n",
    "\n",
    "\n",
    "\n",
    "##print(EVBG_df.shape)\n",
    "\n",
    "\n",
    "EVBG_df['dupe']= EVBG_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "EVBG_df = EVBG_df[(EVBG_df.EVBG == True) | (EVBG_df.Everbridge == True)]\n",
    "\n",
    "\n",
    "print(\"EVBG_df just read shape is \", EVBG_df.shape)\n",
    "\n",
    "\n",
    "EVBG_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'EVBG', 'Everbridge', 'dupe']\n",
    "\n",
    "EVBG_bob.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "EVBG_bob = EVBG_bob.drop(['rowz'], axis = 1)\n",
    "print(\"EVBG_bob shape is \\n\", EVBG_bob.shape)\n",
    "##print(\"Bob head is \\n\", KMI_bob.head())\n",
    "\n",
    "EVBG_rob = EVBG_bob.append(EVBG_df, ignore_index = True)\n",
    "print(\"EVBG_rob shape is \\n\", EVBG_rob.shape)\n",
    "\n",
    "\n",
    "EVBG_rob = EVBG_rob[(EVBG_rob.EVBG == True) | (EVBG_rob.Everbridge == True)]\n",
    "EVBG_rob['dupe']= EVBG_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "##print(\"Bob head is \\n\", rob.head(1))\n",
    "EVBG_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_df_json.csv', header = None)\n",
    "\n",
    "print(\"the EVBG_rob columns are \", EVBG_rob.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charlotte's Web Holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWBHF_df just read shape is  (29, 9)\n",
      "CWBHF_bob shape is \n",
      " (1925, 10)\n",
      "CWBHF shape is \n",
      " (1954, 10)\n",
      "the CWBHF_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'CWBHF', 'Charlottes',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### Charlotte's Web Holdings\n",
    "\n",
    "search_words = 'Charlottes Web Holding'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_CWBHF = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_CWBHF.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'CWBHF'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_CWBHF.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "       \n",
    "\n",
    "colnames_CWBHF = [ 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count']\n",
    "\n",
    "CWBHF_df = pd.DataFrame(tweets_CWBHF, columns = colnames_CWBHF)\n",
    "\n",
    "for i in range(len(CWBHF_df)):\n",
    "    CWBHF_df.loc[i,'CWBHF'] = 'CWBHF' in CWBHF_df.loc[i,'full_text']\n",
    "    CWBHF_df.loc[i,'Charlottes'] = 'Charlottes Web Holding' in CWBHF_df.loc[i,'full_text']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CWBHF_df['dupe']= CWBHF_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CWBHF_df = CWBHF_df[(CWBHF_df.Charlottes == True) | (CWBHF_df.CWBHF == True)]\n",
    "\n",
    "\n",
    "print(\"CWBHF_df just read shape is \", CWBHF_df.shape)\n",
    "\n",
    "\n",
    "CWBHF_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CWBHF_df_json.csv', header = None)\n",
    "columns = ['rowz', 'number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count',  'CWBHF', 'Charlottes','dupe']\n",
    "\n",
    "CWBHF_bob.columns = columns\n",
    "\n",
    "\n",
    "CWBHF_bob = CWBHF_bob.drop(['rowz'], axis = 1)\n",
    "print(\"CWBHF_bob shape is \\n\", CWBHF_bob.shape)\n",
    "##print(\"CWBHF head is \\n\", CWBHF_bob.head())\n",
    "\n",
    "CWBHF_rob = CWBHF_bob.append(CWBHF_df, ignore_index = True)\n",
    "print(\"CWBHF shape is \\n\", CWBHF_rob.shape)\n",
    "\n",
    "CWBHF_rob = CWBHF_rob[(CWBHF_rob.Charlottes == True) | (CWBHF_rob.CWBHF == True)]\n",
    "CWBHF_rob['dupe']= CWBHF_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "##print(\"CWBHF_rob head is \\n\", CWBHF_rob.head(1))\n",
    "CWBHF_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CWBHF_df_json.csv', header = None)\n",
    "\n",
    "print(\"the CWBHF_rob columns are \", CWBHF_rob.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHOP just added is  (100, 8)\n",
      "SHOP_bob shape is \n",
      " (304440, 10)\n",
      "SHOP_rob shape is \n",
      " (304493, 10)\n",
      "the SHOP_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'SHOP', 'Shopify',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "search_words = 'Shopify'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_SHOP = []\n",
    "for tweet in tweets:\n",
    "    tweets_SHOP.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "\n",
    "search_words = 'SHOP'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_CWBHF.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "       \n",
    "\n",
    "        \n",
    "colnames_SHOP = [ 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count']\n",
    "\n",
    "SHOP_df = pd.DataFrame(tweets_SHOP, columns = colnames_SHOP) \n",
    "\n",
    "\n",
    "for i in range(len(SHOP_df)):\n",
    "    SHOP_df.loc[i,'SHOP'] = 'SHOP' in SHOP_df.loc[i,'full_text']\n",
    "    SHOP_df.loc[i,'Shopify'] = 'Shopify' in SHOP_df.loc[i,'full_text']\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"SHOP just added is \", SHOP_df.shape)\n",
    "\n",
    "\n",
    "SHOP_df['dupe']= SHOP_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "\n",
    "SHOP_df = SHOP_df[(SHOP_df.Shopify == True) | (SHOP_df.SHOP == True)]\n",
    "\n",
    "SHOP_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SHOP_df_json.csv', header = None)\n",
    "columns = ['rowz', 'number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'SHOP', 'Shopify','dupe']\n",
    "\n",
    "SHOP_bob.columns = columns\n",
    "\n",
    "\n",
    "SHOP_bob = SHOP_bob.drop(['rowz'], axis = 1)\n",
    "print(\"SHOP_bob shape is \\n\", SHOP_bob.shape)\n",
    "##print(\"CWBHF head is \\n\", CWBHF_bob.head())\n",
    "\n",
    "SHOP_rob = SHOP_bob.append(SHOP_df, ignore_index = True)\n",
    "print(\"SHOP_rob shape is \\n\", SHOP_rob.shape)\n",
    "\n",
    "SHOP_rob = SHOP_rob[(SHOP_rob.Shopify == True) | (SHOP_rob.SHOP == True)]\n",
    "SHOP_rob['dupe']= SHOP_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "##print(\"CWBHF_rob head is \\n\", CWBHF_rob.head(1))\n",
    "SHOP_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SHOP_df_json.csv', header = None)\n",
    "\n",
    "\n",
    "\n",
    "print(\"the SHOP_rob columns are \", SHOP_rob.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanofy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanofy just added is  (100, 8)\n",
      "SNY_bob shape is \n",
      " (14, 10)\n",
      "SNY_rob shape is \n",
      " (14, 10)\n",
      "the SNY_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'SNY', 'Sanofy',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### And now Sanofy\n",
    "\n",
    "\n",
    "search_words = 'SNY'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_SNY = []\n",
    "for tweet in tweets:\n",
    "    tweets_SNY.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'Sanofy'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_SNY.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    " \n",
    "      \n",
    "SNY_df = pd.DataFrame(tweets_SHOP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(SNY_df)):\n",
    "    SNY_df.loc[i,'SNY'] = 'SNY' in SNY_df.loc[i,'full_text']\n",
    "    SNY_df.loc[i,'Sanofy'] = 'Sanofy' in SNY_df.loc[i,'full_text']\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Sanofy just added is \", SNY_df.shape)\n",
    "\n",
    "\n",
    "SNY_df['dupe']= SNY_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SNY_df = SNY_df[(SNY_df.SNY == True) | (SNY_df.Sanofy == True)]\n",
    "\n",
    "SNY_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'SNY', 'Sanofy', 'dupe']\n",
    "\n",
    "SNY_bob.columns = columns\n",
    "\n",
    "\n",
    "SNY_bob = SNY_bob.drop(['rowz'], axis = 1)\n",
    "print(\"SNY_bob shape is \\n\", SNY_bob.shape)\n",
    "##print(\"SNY head is \\n\", SNY_bob.head())\n",
    "\n",
    "SNY_rob = SNY_bob.append(SNY_df, ignore_index = True)\n",
    "print(\"SNY_rob shape is \\n\", SNY_rob.shape)\n",
    "\n",
    "SNY_rob = SNY_rob[(SNY_rob.Sanofy == True) | (SNY_rob.SNY == True)]\n",
    "SNY_rob['dupe']= SNY_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "SNY_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_df_json.csv', header = None)\n",
    "\n",
    "\n",
    "\n",
    "print(\"the SNY_rob columns are \", SNY_rob.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appian just added is  (100, 8)\n",
      "APPN_bob shape is \n",
      " (9, 10)\n",
      "APPN_rob shape is \n",
      " (9, 10)\n",
      "the APPN_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'APPN', 'Appian',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "### And now Appian\n",
    "\n",
    "\n",
    "search_words = 'APPN'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_APPN = []\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'Appian'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    " \n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "APPN_df = pd.DataFrame(tweets_SHOP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(APPN_df)):\n",
    "    APPN_df.loc[i,'APPN'] = 'APPN' in APPN_df.loc[i,'full_text']\n",
    "    APPN_df.loc[i, 'Appian'] = 'Appian' in APPN_df.loc[i,'full_text']\n",
    "    \n",
    "\n",
    "\n",
    "print(\"Appian just added is \", APPN_df.shape)\n",
    "\n",
    "\n",
    "APPN_df['dupe']= APPN_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "\n",
    "APPN_df = APPN_df[(APPN_df.APPN == True) | (APPN_df.Appian == True)]\n",
    "\n",
    "\n",
    "APPN_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_df_json.csv', header = None)\n",
    "columns = ['rowz', 'number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'APPN', 'Appian', 'dupe']\n",
    "\n",
    "APPN_bob.columns = columns\n",
    "\n",
    "\n",
    "APPN_bob = APPN_bob.drop(['rowz'], axis = 1)\n",
    "print(\"APPN_bob shape is \\n\", APPN_bob.shape)\n",
    "##print(\"SNY head is \\n\", SNY_bob.head())\n",
    "\n",
    "APPN_rob = APPN_bob.append(APPN_df, ignore_index = True)\n",
    "print(\"APPN_rob shape is \\n\", APPN_rob.shape)\n",
    "\n",
    "APPN_rob = APPN_rob[(APPN_rob.APPN == True) | (APPN_rob.Appian == True)]\n",
    "APPN_rob['dupe']= APPN_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "APPN_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_df_json.csv', header = None)\n",
    "\n",
    "print(\"the APPN_rob columns are \", APPN_rob.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspire Medical Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspire just added is  (1, 8)\n",
      "INSP_bob shape is \n",
      " (1179, 10)\n",
      "INSP_rob shape is \n",
      " (1179, 10)\n",
      "the INSP_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'INSP', 'Inspire_M_S',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tweets_INSP = []\n",
    "\n",
    "tweets_INSP.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    " \n",
    "search_words = 'Inspire Medical Systems'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'INSP'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "    \n",
    "INSP_df = pd.DataFrame(tweets_INSP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(INSP_df)):\n",
    "    INSP_df.loc[i,'INSP'] = 'INSP' in INSP_df.loc[i,'full_text']\n",
    "    INSP_df.loc[i,'Inspire_M_S'] = 'Inspire Medical' in INSP_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_INSP = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'INSP', 'Inspire_M_S']\n",
    "\n",
    "\n",
    "\n",
    "print(\"Inspire just added is \", INSP_df.shape)\n",
    "\n",
    "\n",
    "INSP_df['dupe']= INSP_df[\"full_text\"].duplicated()\n",
    "\n",
    "    \n",
    "INSP_df = INSP_df[(INSP_df.INSP == True) | (INSP_df.Inspire_M_S == True)]\n",
    "\n",
    "INSP_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'INSP', 'Inspire_M_S', 'dupe']\n",
    "\n",
    "INSP_bob.columns = columns\n",
    "\n",
    "\n",
    "INSP_bob = INSP_bob.drop(['rowz'], axis = 1)\n",
    "print(\"INSP_bob shape is \\n\", INSP_bob.shape)\n",
    "##print(\"INSP head is \\n\", INSP_bob.head())\n",
    "\n",
    "INSP_rob = INSP_bob.append(INSP_df, ignore_index = True)\n",
    "print(\"INSP_rob shape is \\n\", INSP_rob.shape)\n",
    "\n",
    "INSP_rob = INSP_rob[(INSP_rob.INSP == True) | (INSP_rob.Inspire_M_S == True)]\n",
    "INSP_rob['dupe']= INSP_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "INSP_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_df_json.csv', header = None)\n",
    "\n",
    "\n",
    "print(\"the INSP_rob columns are \", INSP_rob.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMED just added is  (16, 8)\n",
      "GMED_bob shape is \n",
      " (13201, 10)\n",
      "GMED_rob shape is \n",
      " (13211, 10)\n",
      "the GMED_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'GMED', 'Globus_M',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "search_words = 'Globus Medical'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_GMED = []\n",
    "for tweet in tweets:\n",
    "    tweets_GMED.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'GMED'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_GMED.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "GMED_df = pd.DataFrame(tweets_GMED, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(GMED_df)):\n",
    "    GMED_df.loc[i,'GMED'] = 'GMED' in GMED_df.loc[i,'full_text']\n",
    "    GMED_df.loc[i,'Globus_M'] = 'Globus' in GMED_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_GMED = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'GMED', 'Globus_M']\n",
    "\n",
    "\n",
    "\n",
    "print(\"GMED just added is \", GMED_df.shape)\n",
    "\n",
    "\n",
    "GMED_df['dupe']= GMED_df[\"full_text\"].duplicated()\n",
    "    \n",
    "GMED_df = GMED_df[(GMED_df.GMED == True) | (GMED_df.Globus_M == True)]\n",
    "\n",
    "\n",
    "GMED_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'GMED', 'Globus_M', 'dupe']\n",
    "\n",
    "GMED_bob.columns = columns\n",
    "\n",
    "\n",
    "GMED_bob = GMED_bob.drop(['rowz'], axis = 1)\n",
    "print(\"GMED_bob shape is \\n\", GMED_bob.shape)\n",
    "##print(\"GMED head is \\n\", IGMED_bob.head())\n",
    "\n",
    "GMED_rob = GMED_bob.append(GMED_df, ignore_index = True)\n",
    "print(\"GMED_rob shape is \\n\", GMED_rob.shape)\n",
    "\n",
    "GMED_rob = GMED_rob[(GMED_rob.GMED == True) | (GMED_rob.Globus_M == True)]\n",
    "GMED_rob['dupe']= GMED_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "GMED_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_df_json.csv', header = None)\n",
    "\n",
    "print(\"the GMED_rob columns are \", GMED_rob.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowdstrike holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crowdstrike just added is  (111, 8)\n",
      "CRWD_bob shape is \n",
      " (21158, 10)\n",
      "CRWD_rob shape is \n",
      " (21217, 10)\n",
      "the CRWD_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'CRWD', 'Crowdstrike',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "search_words = 'Crowdstrike Holdings'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_CRWD = []\n",
    "for tweet in tweets:\n",
    "    tweets_CRWD.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'CRWD'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_CRWD.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "CRWD_df = pd.DataFrame(tweets_CRWD, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(CRWD_df)):\n",
    "    CRWD_df.loc[i,'CRWD'] = 'CRWD' in CRWD_df.loc[i,'full_text']\n",
    "    CRWD_df.loc[i,'Crowdstrike'] = 'Crowdstrike' in CRWD_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_CRWD = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_CRWD', 'has_Crowdstrike']\n",
    "\n",
    "\n",
    "\n",
    "print(\"Crowdstrike just added is \",CRWD_df.shape)\n",
    "\n",
    "\n",
    "CRWD_df['dupe']= CRWD_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "    \n",
    "CRWD_df = CRWD_df[(CRWD_df.CRWD == True) | (CRWD_df.Crowdstrike == True)]\n",
    "\n",
    "CRWD_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_df_json.csv', header = None)\n",
    "columns = ['rowz','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'CRWD', 'Crowdstrike', 'dupe']\n",
    "\n",
    "CRWD_bob.columns = columns\n",
    "\n",
    "\n",
    "CRWD_bob = CRWD_bob.drop(['rowz'], axis = 1)\n",
    "print(\"CRWD_bob shape is \\n\", CRWD_bob.shape)\n",
    "##print(\"GCRWD head is \\n\", CRWD_bob.head())\n",
    "\n",
    "CRWD_rob = CRWD_bob.append(CRWD_df, ignore_index = True)\n",
    "print(\"CRWD_rob shape is \\n\", CRWD_rob.shape)\n",
    "\n",
    "CRWD_rob = CRWD_rob[(CRWD_rob.CRWD == True) | (CRWD_rob.Crowdstrike == True)]\n",
    "CRWD_rob['dupe']= CRWD_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "CRWD_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_df_json.csv', header = None)\n",
    "\n",
    "print(\"the CRWD_rob columns are \", CRWD_rob.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Exxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exxon just read is  (199, 8)\n",
      "XOM_bob shape is \n",
      " (196600, 10)\n",
      "XOM_rob shape is \n",
      " (196703, 10)\n",
      "the XOM_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'XOM', 'Exxon',\n",
      "       'dupe'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "search_words = 'Exxon'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "  \n",
    "tweets_Exxon = []\n",
    "for tweet in tweets:\n",
    "    tweets_Exxon.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'XOM'\n",
    "\n",
    "##tweets = api.search_tweets(q = search_words, lang = \"en\", count = items_to_get )\n",
    "\n",
    "tweets = api.search_30_day(label = 'ThirtyDaze', query = search_words, fromDate = minus_one, toDate = whats_today,\n",
    "                           maxResults = items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_Exxon.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "XOM_df = pd.DataFrame(tweets_Exxon, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(XOM_df)):\n",
    "    XOM_df.loc[i,'XOM'] = 'XOM' in XOM_df.loc[i,'full_text']\n",
    "    XOM_df.loc[i,'Exxon'] = 'Exxon' in XOM_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_XOM = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count',  'XOM','Exxon',]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Exxon just read is \", XOM_df.shape)\n",
    "\n",
    "\n",
    "XOM_df['dupe']= XOM_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "    \n",
    "XOM_df = XOM_df[(XOM_df.Exxon == True) | (XOM_df.XOM == True)]\n",
    "\n",
    "XOM_bob = pd.read_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/XOM_df_json.csv', header = None)\n",
    "columns = ['rowz', 'number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'XOM','Exxon',  'dupe']\n",
    "\n",
    "XOM_bob.columns = columns\n",
    "\n",
    "\n",
    "XOM_bob = XOM_bob.drop(['rowz'], axis = 1)\n",
    "print(\"XOM_bob shape is \\n\", XOM_bob.shape)\n",
    "##print(\"XOM head is \\n\", XOM_bob.head())\n",
    "\n",
    "XOM_rob = XOM_bob.append(XOM_df, ignore_index = True)\n",
    "print(\"XOM_rob shape is \\n\", XOM_rob.shape)\n",
    "\n",
    "XOM_rob = XOM_rob[(XOM_rob.XOM == True) | (XOM_rob.Exxon == True)]\n",
    "XOM_rob['dupe']= XOM_rob['full_text'].duplicated(keep = 'last')\n",
    "\n",
    "\n",
    "XOM_rob.to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/XOM_df_json.csv', header = None)\n",
    "\n",
    "print(\"the XOM_rob columns are \", XOM_rob.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the AMZN_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'AMZN', 'Amazon',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the AMZN_rob full text row 1  RT @BloombergQuint: Amazon will show made-in-China labels for India as tensions rise.\n",
      "\n",
      "Read more: https://t.co/SiMMYnmTHj https://t.co/rlIu‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ\n",
      "the KMI_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'KMI', 'Kinder_Morgan',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the KMI_rob full text row 1  RT @Trading_Sunset: Kinder Morgan $KMI settled -0.6% to $16.75. Bearish engulfing candle... leans s/t bearish. Every cent lower is a bonus.…\n",
      "the EVBG_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'EVBG', 'Everbridge',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the EVBG_rob full text row 1  RT @Everbridge: UK and Europe: \n",
      "On September 16 at 12PM GMT join our webinar Managing Multiple Threats: COVID-19 and Beyond.\n",
      "Gain insights‚Ä¶\n",
      "the SHOP_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'SHOP', 'Shopify',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the SHOP_rob full text row 1  RT @__xuorig__: Going live in 1 hour with @swalkinshaw from @ShopifyEngü§òIf anyone knows how to #graphql, it's the folks from Shopify, espec‚Ä¶\n",
      "the APPN_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'APPN', 'Appian',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the APPN_rob full text row 1  RT @jaminball: Appian has posted the 5th highest YTD returns of all cloud stocks this year yet they often get overlooked. Founded in the 90‚Ä¶\n",
      "the INSP_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'INSP', 'Inspire_M_S',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the INSP_rob full text row 1  Inspire Medical Systems $INSP Price Target Raised to $137.00 https://t.co/VA6kKNNCAV #stocks\n",
      "the GMED_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'GMED', 'Globus_M',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the GMED_rob full text row 1  The Zacks Analyst Blog Highlights: Teradyne, ABB, Brooks Automation and Globus Medical\n",
      " https://t.co/6Gjww0kus5\n",
      "the CRWD_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'CRWD', 'Crowdstrike',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the CRWD_rob full text row 1  Closed $CRWD CrowdStrike Holdings position for a loss (-3.8%)\n",
      "the XOM_rob columns are  Index(['number', 'created_at', 'full_text', 'retweet_count', 'user_id',\n",
      "       'user.favourites_count', 'user.followers_count', 'XOM', 'Exxon',\n",
      "       'dupe'],\n",
      "      dtype='object')\n",
      "the XOM_rob full text row 1  RT @Reuters: EXCLUSIVE New York state pension fund backs activist nominees in Exxon proxy fight https://t.co/Iuf87VRHlD https://t.co/0Xks7H‚Ä¶\n",
      "the time is  2022-03-24 17:41:58.876116\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"the AMZN_rob columns are \", AMZN_rob.columns)\n",
    "print(\"the AMZN_rob full text row 1 \", AMZN_rob.full_text[1])\n",
    "\n",
    "print(\"the KMI_rob columns are \", KMI_rob.columns)\n",
    "print(\"the KMI_rob full text row 1 \", KMI_rob.full_text[1])\n",
    "\n",
    "print(\"the EVBG_rob columns are \", EVBG_rob.columns)\n",
    "print(\"the EVBG_rob full text row 1 \", EVBG_rob.full_text[1])\n",
    "\n",
    "print(\"the SHOP_rob columns are \", SHOP_rob.columns)\n",
    "print(\"the SHOP_rob full text row 1 \", SHOP_rob.full_text[1])\n",
    "\n",
    "\n",
    "print(\"the APPN_rob columns are \", APPN_rob.columns)\n",
    "print(\"the APPN_rob full text row 1 \", APPN_rob.full_text[1])\n",
    "\n",
    "print(\"the INSP_rob columns are \", INSP_rob.columns)\n",
    "print(\"the INSP_rob full text row 1 \", INSP_rob.full_text[1])\n",
    "\n",
    "print(\"the GMED_rob columns are \", GMED_rob.columns)\n",
    "print(\"the GMED_rob full text row 1 \", GMED_rob.full_text[1])\n",
    "\n",
    "print(\"the CRWD_rob columns are \", CRWD_rob.columns)\n",
    "print(\"the CRWD_rob full text row 1 \", CRWD_rob.full_text[1])\n",
    "\n",
    "print(\"the XOM_rob columns are \", XOM_rob.columns)\n",
    "print(\"the XOM_rob full text row 1 \", XOM_rob.full_text[1])\n",
    "\n",
    "\n",
    "whats_now = datetime.datetime.now()\n",
    "\n",
    "print(\"the time is \", whats_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c92656ff0f0fa4c3a42115786bbd1b0f37638d3882483cafaff4ad320c2e529"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('TFlow': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
