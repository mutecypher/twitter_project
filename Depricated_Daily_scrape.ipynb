{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Twitter Scape as of September 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8 (default, Apr 13 2021, 12:59:45) \n",
      "[Clang 10.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Created on Mon Apr 20 11:47:44 2020\n",
    "\n",
    "@author: mutecypher\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy \n",
    "import json\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "consumer_key = \"hRA7EQ2Xem4WW4a0EFqQOasZY\"\n",
    "consumer_secret = \"ZRLOHC1ercuD8u35RjpbqF39GlmC4J5ekTVAYrdOEDaQdG7FLp\"\n",
    "access_token = \"877329134061596672-7JysRR5QnZsf2LQFLUk1q7aWXUQq4CP\"\n",
    "access_token_secret = \"iShX6tt9P4JebyiXf1S8z07TCeOwoWGRe4aytGYcFBjdZ\"\n",
    "API_key = 'Bj9jB1oWpjNGfAurcSA7vNkj0'\n",
    "API_SECRET_KEY = '3f6Lv4W6s4soRMNmdZflaPWrEDp41cugo3EyOqDCuDCzoSuQMT'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAALeyOgEAAAAAEQNu9C0zdTLIWdJVL2cZi6G3WWM%3DjaeSOIcT9bW6GSWwovcbld7Gwe8zXugDmCk660m0yMPuavP7vZ'\n",
    "##auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "##auth = tweepy.AppAuthHandler(bearer_token, consumer_secret)\n",
    "##auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "##\n",
    "auth = tweepy.OAuthHandler(API_key, API_SECRET_KEY)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "##\n",
    "\n",
    "\n",
    "items_to_get = 3000\n",
    "\n",
    "whats_today = datetime.datetime.now()\n",
    "minus_one = whats_today - datetime.timedelta(days = 2)\n",
    "minus_two = whats_today - datetime.timedelta(days = 600)\n",
    "\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'AMZN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9429c977dbfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mAmazon_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAmazon_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAmazon_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAMZN\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAmazon_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAmazon\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mAmazon_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/Amazon_df_json.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'AMZN'"
     ]
    }
   ],
   "source": [
    "## Get the Amazon tweets\n",
    "\n",
    "search_words = 'Amazon'\n",
    "\n",
    "\n",
    "tweets = api.search_tweets(q=search_words, lang =\"en\", \n",
    "                           since_id=minus_one, max_id = items_to_get)\n",
    "\n",
    "##tweets = tweepy.Cursor(api.search(q=search_words),\n",
    "##              lang=\"en\",\n",
    " ##             since =minus_one).items(items_to_get)\n",
    "\n",
    "tweets_AMZN = []\n",
    "for tweet in tweets:\n",
    "    tweets_AMZN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    print(tweet)\n",
    "\n",
    "\n",
    "search_words = 'AMZN'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##tweepy.Cursor(api.search,\n",
    "##              q=search_words,\n",
    "##              lang=\"en\",\n",
    "##              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_AMZN.append((tweet.created_at, tweet.text, \n",
    "                        tweet.retweet_count, tweet.user.id,  \n",
    "                        tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "Amazon_df = pd.DataFrame(tweets_AMZN, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(Amazon_df)):\n",
    "    Amazon_df.loc[i,'AMZN'] = 'AMZN' in Amazon_df.loc[i,'full_text']\n",
    "    Amazon_df.loc[i,'Amazon'] = 'Amazon' in Amazon_df.loc[i,'full_text']\n",
    "    \n",
    "Amazon_df['dupe']= Amazon_df[ \"full_text\"].duplicated()\n",
    "    \n",
    "\n",
    "\n",
    "##print(\"Number of AMZN duplicates overall is\", Amazon_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_Am_df = Amazon_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_Am_df = no_dupe_Am_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_Am_df = no_dupe_Am_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"After removing duplicate rows, the AMZN shape is \", no_dupe_Am_df.shape)\n",
    "\n",
    "\n",
    "Amazon_df = Amazon_df[(Amazon_df.AMZN == True) | (Amazon_df.Amazon == True)]\n",
    "Amazon_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/Amazon_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_Am_df.to_csv('AMZN_no_dupes_df.csv', mode = 'a', header = False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kinder Morgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_words = 'Kinder Morgan'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_KMI = []\n",
    "for tweet in tweets:\n",
    "    tweets_KMI.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'KMI'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_KMI.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "KMI_df = pd.DataFrame(tweets_KMI, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(KMI_df)):\n",
    "    KMI_df.loc[i,'KMI'] = 'KMI' in KMI_df.loc[i,'full_text']\n",
    "    KMI_df.loc[i,'Kinder_Morgan'] = 'Kinder_Morgan' in KMI_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_KMI = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_KMI', 'has_Kinder_morgan']\n",
    "\n",
    "\n",
    "\n",
    "##print(KMI_df.shape)\n",
    "\n",
    "\n",
    "KMI_df['dupe']= KMI_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of KMI duplicates overall is\", KMI_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_KMI_df = KMI_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_KMI_df = no_dupe_KMI_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_KMI_df = no_dupe_KMI_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the KMI shape is \", no_dupe_KMI_df.shape)\n",
    "\n",
    "\n",
    "    \n",
    "KMI_df = KMI_df[(KMI_df.KMI == True) | (KMI_df.Kinder_Morgan == True)]\n",
    "KMI_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/KMI_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_KMI_df.to_csv('KMI_no_dupes_df.csv',mode = 'a', header = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everbridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "search_words = 'Everbridge'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_EVBG = []\n",
    "for tweet in tweets:\n",
    "    tweets_EVBG.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    " \n",
    "search_words = 'EVBG'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_EVBG.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "     \n",
    "    \n",
    "EVBG_df = pd.DataFrame(tweets_EVBG, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(EVBG_df)):\n",
    "    EVBG_df.loc[i,'EVBG'] = 'EVBG' in EVBG_df.loc[i,'full_text']\n",
    "    EVBG_df.loc[i,'Everbridge'] = 'Everbridge' in EVBG_df.loc[i,'full_text']\n",
    "    \n",
    "    \n",
    "colnames_EVBG = ['number', 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_EVBG', 'has_Everbridge']\n",
    "\n",
    "\n",
    "\n",
    "##print(EVBG_df.shape)\n",
    "\n",
    "\n",
    "EVBG_df['dupe']= EVBG_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of EVBG duplicates overall is\", EVBG_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_EVBG_df = EVBG_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_EVBG_df = no_dupe_EVBG_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_EVBG_df = no_dupe_EVBG_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\n After removing duplicate rows, the EVBG shape is \", no_dupe_EVBG_df.shape)\n",
    "\n",
    "\n",
    "EVBG_df = EVBG_df[(EVBG_df.EVBG == True) | (EVBG_df.Everbridge == True)]\n",
    "EVBG_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/EVBG_df_json.csv',  mode = 'a', header = False)\n",
    "\n",
    "\n",
    "\n",
    "##no_dupe_EVBG_df.to_csv('EVBG_no_dupes_df.csv', mode = 'a',header = False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charlotte's Web Holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Charlotte's Web Holdings\n",
    "\n",
    "search_words = 'Charlottes Web Holding'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_CWBHF = []\n",
    "for tweet in tweets:\n",
    "    tweets_CWBHF.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    " \n",
    "search_words = 'CWBHF'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_CWBHF.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "       \n",
    "\n",
    "colnames_CWBHF = [ 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count']\n",
    "\n",
    "CWBHF_df = pd.DataFrame(tweets_CWBHF, columns = colnames_CWBHF)\n",
    "\n",
    "for i in range(len(CWBHF_df)):\n",
    "    CWBHF_df.loc[i,'Charlottes'] = 'Charlottes Web Holding' in CWBHF_df.loc[i,'full_text']\n",
    "    CWBHF_df.loc[i,'CWBHF'] = 'CWBHF' in CWBHF_df.loc[i,'full_text']\n",
    "\n",
    "\n",
    "\n",
    "##print(CWBHF_df.shape)\n",
    "\n",
    "\n",
    "CWBHF_df['dupe']= CWBHF_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of CWBHF duplicates overall is\", CWBHF_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_CWBHF_df = CWBHF_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_CWBHF_df = no_dupe_CWBHF_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_CWBHF_df = no_dupe_CWBHF_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the CWBHF shape is \", no_dupe_CWBHF_df.shape)\n",
    "\n",
    "\n",
    "CWBHF_df = CWBHF_df[(CWBHF_df.Charlottes == True) | (CWBHF_df.CWBHF == True)]\n",
    "CWBHF_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/CWBHF_df_json.csv',  mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_CWBHF_df.to_csv('CWBHF_no_dupes_df.csv', mode = 'a',header = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shopify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_words = 'Shopify'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_SHOP = []\n",
    "for tweet in tweets:\n",
    "    tweets_SHOP.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "colnames_SHOP = [ 'created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count']\n",
    "\n",
    "SHOP_df = pd.DataFrame(tweets_SHOP, columns = colnames_SHOP) \n",
    "\n",
    "for i in range(len(SHOP_df)):\n",
    "    SHOP_df.loc[i,'Shopify'] = 'Shopify' in SHOP_df.loc[i,'full_text']\n",
    "    \n",
    "    \n",
    "\n",
    "##print(SHOP_df.shape)\n",
    "\n",
    "\n",
    "SHOP_df['dupe']= SHOP_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of SHOP duplicates overall is\", SHOP_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "\n",
    "\n",
    "##no_dupe_SHOP_df = SHOP_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_SHOP_df = no_dupe_SHOP_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_SHOP_df = no_dupe_SHOP_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the SHOP shape is \", no_dupe_SHOP_df.shape)\n",
    "\n",
    "\n",
    "##no_dupe_SHOP_df.to_csv('SHOP_no_dupes_df.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "\n",
    "SHOP_df = SHOP_df[SHOP_df.Shopify == True]\n",
    "SHOP_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/SHOP_df_json.csv',  mode = 'a', header = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanofy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### And now Sanofy\n",
    "\n",
    "\n",
    "search_words = 'SNY'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_SNY = []\n",
    "for tweet in tweets:\n",
    "    tweets_SNY.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'Sanofy'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_SNY.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    " \n",
    "      \n",
    "SNY_df = pd.DataFrame(tweets_SHOP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(SNY_df)):\n",
    "    SNY_df.loc[i,'SNY'] = 'SNY' in SNY_df.loc[i,'full_text']\n",
    "    SNY_df.loc[i,'Sanofy'] = 'Sanofy' in SNY_df.loc[i,'full_text']\n",
    "    \n",
    "    \n",
    "## colnames_SNY = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_SNY']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##print(SNY_df.shape)\n",
    "\n",
    "\n",
    "SNY_df['dupe']= SNY_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of SNY duplicates overall is\", SNY_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_SNY_df = SNY_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_SNY_df = no_dupe_SNY_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_SNY_df = no_dupe_SNY_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the SNY shape is \", no_dupe_SNY_df.shape)\n",
    "\n",
    "\n",
    "##no_dupe_SNY_df.to_csv('SNY_no_dupes_df.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "\n",
    "SNY_df_tru = SNY_df[(SNY_df.SNY == True) | (SNY_df.Sanofy == True)]\n",
    "SNY_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/SNY_df_json.csv',  mode = 'a', header = False)\n",
    "\n",
    "\n",
    "print(\" SNY_df shape is \", SNY_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### And now Appian\n",
    "\n",
    "\n",
    "search_words = 'APPN'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_APPN = []\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'Appian'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    " \n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "APPN_df = pd.DataFrame(tweets_SHOP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(APPN_df)):\n",
    "    APPN_df.loc[i,'APPN'] = 'APPN' in APPN_df.loc[i,'full_text']\n",
    "    APPN_df.loc[i, 'Appian'] = 'Appian' in APPN_df.loc[i,'full_text']\n",
    "    \n",
    "\n",
    " \n",
    "##colnames_APPN = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_APPN', 'has_Appian']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##print(APPN_df.shape)\n",
    "\n",
    "\n",
    "APPN_df['dupe']= APPN_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of APPN duplicates overall is\", APPN_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_APPN_df = APPN_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_APPN_df = no_dupe_APPN_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_APPN_df = no_dupe_APPN_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the APPN shape is \", no_dupe_SNY_df.shape)\n",
    "\n",
    "\n",
    "##no_dupe_APPN_df.to_csv('APPN_no_dupes_df.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "\n",
    "APPN_df_tru = APPN_df[(APPN_df.APPN == True) | (APPN_df.Appian == True)]\n",
    "APPN_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/APPN_df_json.csv',  mode = 'a', header = False)\n",
    "\n",
    "print(\"APPN shape is \", APPN_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspire Medical Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_INSP = []\n",
    "\n",
    "tweets_INSP.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    " \n",
    "search_words = 'Inspire Medical Systems'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_APPN.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'INSP'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)   \n",
    "    \n",
    "INSP_df = pd.DataFrame(tweets_INSP, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(INSP_df)):\n",
    "    INSP_df.loc[i,'INSP'] = 'INSP' in INSP_df.loc[i,'full_text']\n",
    "    INSP_df.loc[i,'Inspire_M_S'] = 'Inspire Medical Systems' in INSP_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_INSP = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_INSP', 'has_Inspire_M_S']\n",
    "\n",
    "\n",
    "\n",
    "##print(INSP_df.shape)\n",
    "\n",
    "\n",
    "INSP_df['dupe']= INSP_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of INSP duplicates overall is\", INSP_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_INSP_df = INSP_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_INSP_df = no_dupe_INSP_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_INSP_df = no_dupe_INSP_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the INSP shape is \", no_dupe_INSP_df.shape)\n",
    "\n",
    "\n",
    "    \n",
    "INSP_df = INSP_df[(INSP_df.INSP == True) | (INSP_df.Inspire_M_S == True)]\n",
    "INSP_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/INSP_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_INSP_df.to_csv('INSP_no_dupes_df.csv',mode = 'a', header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = 'Globus Medical'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_GMED = []\n",
    "for tweet in tweets:\n",
    "    tweets_GMED.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'GMED'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "for tweet in tweets:\n",
    "    tweets_GMED.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "GMED_df = pd.DataFrame(tweets_GMED, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(GMED_df)):\n",
    "    GMED_df.loc[i,'GMED'] = 'GMED' in GMED_df.loc[i,'full_text']\n",
    "    GMED_df.loc[i,'Globus_M'] = 'Globus' in GMED_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_GMED = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_GMED', 'has_Globus_M']\n",
    "\n",
    "\n",
    "\n",
    "##print(GMED_df.shape)\n",
    "\n",
    "\n",
    "GMED_df['dupe']= GMED_df[\"full_text\"].duplicated()\n",
    "\n",
    "\n",
    "##no_dupe_GMED_df = GMED_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_GMED_df = no_dupe_GMED_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_GMED_df = no_dupe_GMED_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the GMED shape is \", no_dupe_GMED_df.shape)\n",
    "\n",
    "\n",
    "    \n",
    "GMED_df = GMED_df[(GMED_df.GMED == True) | (GMED_df.Globus_M == True)]\n",
    "GMED_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/GMED_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_GMED_df.to_csv('GMED_no_dupes_df.csv',mode = 'a', header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crowdstrike holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = 'Crowdstrike Holdings'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_CRWD = []\n",
    "for tweet in tweets:\n",
    "    tweets_CRWD.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'CRWD'\n",
    "\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_CRWD.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "CRWD_df = pd.DataFrame(tweets_CRWD, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(CRWD_df)):\n",
    "    CRWD_df.loc[i,'CRWD'] = 'CRWD' in CRWD_df.loc[i,'full_text']\n",
    "    CRWD_df.loc[i,'Crowdstrike'] = 'Crowdstrike' in CRWD_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_CRWD = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_CRWD', 'has_Crowdstrike']\n",
    "\n",
    "\n",
    "\n",
    "##print(CRWD_df.shape)\n",
    "\n",
    "\n",
    "CRWD_df['dupe']= CRWD_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of CRWD duplicates overall is\", CRWD_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_CRWD_df = CRWD_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_CRWD_df = no_dupe_CRWD_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_CRWD_df = no_dupe_CRWD_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the CRWD shape is \", no_dupe_CRWD_df.shape)\n",
    "\n",
    "\n",
    "    \n",
    "CRWD_df = CRWD_df[(CRWD_df.CRWD == True) | (CRWD_df.Crowdstrike == True)]\n",
    "CRWD_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/CRWD_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_CRWD_df.to_csv('CRWD_no_dupes_df.csv',mode = 'a', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CRWD_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = 'Exxon'\n",
    "\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since_id=minus_one).items(items_to_get)\n",
    "  \n",
    "tweets_Exxon = []\n",
    "for tweet in tweets:\n",
    "    tweets_Exxon.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "search_words = 'XOM'\n",
    "\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweets_Exxon.append((tweet.created_at, tweet.text, tweet.retweet_count, tweet.user.id,  tweet.user.favourites_count, tweet.user.followers_count ))\n",
    "    \n",
    "    \n",
    "XOM_df = pd.DataFrame(tweets_Exxon, columns = ['created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count'])\n",
    "\n",
    "\n",
    "for i in range(len(XOM_df)):\n",
    "    XOM_df.loc[i,'Exxon'] = 'Exxon' in XOM_df.loc[i,'full_text']\n",
    "    XOM_df.loc[i,'XOM'] = 'XOM' in XOM_df.loc[i,'full_text']\n",
    "    \n",
    "  \n",
    "    \n",
    "colnames_XOM = ['number', 'created_at','text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count', 'has_Exxon', 'has_XOM']\n",
    "\n",
    "\n",
    "\n",
    "##print(XOM_df.shape)\n",
    "\n",
    "\n",
    "XOM_df['dupe']= XOM_df[\"full_text\"].duplicated()\n",
    "\n",
    "##print(\"Number of CRWD duplicates overall is\", CRWD_df[\"dupe\"].value_counts()[1])\n",
    "\n",
    "##no_dupe_Am_df = Am_df[Am_df[\"dupe\"] == True]\n",
    "\n",
    "##no_dupe_XOM_df = XOM_df.drop_duplicates(subset = \"full_text\", keep = 'last')\n",
    "\n",
    "##no_dupe_XOM_df = no_dupe_XOM_df.drop(\"dupe\", 1)\n",
    "\n",
    "##no_dupe_XOM_df = no_dupe_XOM_df.reset_index(drop = True, inplace = False)\n",
    "\n",
    "##print(\"\\nAfter removing duplicate rows, the XOM shape is \", no_dupe_XOM_df.shape)\n",
    "\n",
    "\n",
    "    \n",
    "XOM_df = XOM_df[(XOM_df.Exxon == True) | (XOM_df.XOM == True)]\n",
    "XOM_df.to_csv('/Volumes/Elements/UW_work/GitHub/twitter-project/Data Files/XOM_df_json.csv', mode = 'a', header = False)\n",
    "\n",
    "\n",
    "##no_dupe_XOM_df.to_csv('XOM_no_dupes_df.csv',mode = 'a', header = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whats_now = datetime.datetime.now()\n",
    "print(\"the time is \", whats_now)\n",
    "print(\"the tail of amazon twitter is \", Amazon_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
