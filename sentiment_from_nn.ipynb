{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the gru neural net model on the various twitter scrapes to get a rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math as math\n",
    "\n",
    "##from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "\n",
    "##Sequential  ##, save_model, load_model\n",
    "from tensorflow.keras.models import save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, ReduceLROnPlateau, TensorBoard, LambdaCallback\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3457: DtypeWarning: Columns (1,4,6,7,8,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the shape of dataframe CRWD_df is (20763, 10)\n",
      "\n",
      "the shape of dataframe APPN_df is (8, 10)\n",
      "\n",
      "the shape of dataframe GMED_df is (13162, 10)\n",
      "\n",
      "the shape of dataframe INSP_df is (1178, 10)\n",
      "\n",
      "the shape of dataframe SNY_df is (13, 10)\n",
      "\n",
      "the shape of dataframe EVBG_df is (137843, 10)\n",
      "\n",
      "the shape of dataframe KMI_df is (6188, 10)\n",
      "\n",
      "the shape of dataframe Am_df is (356934, 10)\n",
      "\n",
      "the shape of dataframe XOM_df is (195919, 10)\n"
     ]
    }
   ],
   "source": [
    "filez = [\n",
    "    '/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_df_json.csv', \n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_df_json.csv',\n",
    "'//Volumes/Elements/GitHub/twitter-project/Data Files/KMI_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_df_json.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/XOM_df_json.csv'\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "i = 0\n",
    "for filename in filez:\n",
    "##    print(\"filename is\", filename)\n",
    "    dataframes.append(pd.read_csv(filename, header=0, index_col=0, parse_dates=True))\n",
    "    i = i+1\n",
    "Krabby = [\n",
    "    'CRWD_df', 'APPN_df',  \n",
    "          'GMED_df',\n",
    "          'INSP_df', 'SNY_df',\n",
    "          'EVBG_df',\n",
    "          'KMI_df',\n",
    "          'Am_df'\n",
    "    , 'XOM_df'\n",
    "]\n",
    "   \n",
    "for i in range(len(Krabby)):\n",
    "    print(\"\\nthe shape of dataframe\", Krabby[i],\"is\", dataframes[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some questions answered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   number     created_at                                          full_text  \\\n",
      "0                                                                             \n",
      "1     1.0  9/17/20 14:44  Closed $CRWD CrowdStrike Holdings position for...   \n",
      "2     2.0   9/15/20 0:00  $CRWD Crowdstrike Holdings Inc (NASDAQ:CRWD) i...   \n",
      "3     3.0  9/14/20 21:29  RT @MonitorAction: $CRWD Michael Carpenter sel...   \n",
      "4     4.0  9/14/20 20:48  $CRWD Michael Carpenter sells $3.9M Worth Of C...   \n",
      "5     5.0  9/14/20 20:31  $CRWD George Kurtz sells $3.3M Worth Of CrowdS...   \n",
      "\n",
      "   retweet_count       user_id  user.favourites_count  user.followers_count  \\\n",
      "0                                                                             \n",
      "1            1.0  1.270000e+18                     16                     5   \n",
      "2            0.0  2.579611e+09                   1818                    42   \n",
      "3            1.0  2.250788e+09                   1481                    57   \n",
      "4            1.0  1.180000e+18                     11                   582   \n",
      "5            0.0  1.180000e+18                     11                   582   \n",
      "\n",
      "   Name  Symbol  dupe  \n",
      "0                      \n",
      "1  True   False  True  \n",
      "2  True    True  True  \n",
      "3  True   False  True  \n",
      "4  True   False  True  \n",
      "5  True   False  True  \n"
     ]
    }
   ],
   "source": [
    "dataframes[0].columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','Name' ,'Symbol', 'dupe']\n",
    "\n",
    "print(dataframes[0].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3q/y2fjlm8n4752m4cv89kq3r0h0000gn/T/ipykernel_28151/2666789012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[1;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     model = model_config_lib.model_from_config(model_config,\n\u001b[1;32m    168\u001b[0m                                                custom_objects=custom_objects)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "model_file = '/Volumes/Elements/GitHub/twitter-project/twitter-project/model.h5'\n",
    "\n",
    "loaded_model = load_model(\n",
    "    model_file,\n",
    "    custom_objects = None,\n",
    "    compile=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_words = 30000\n",
    "pad = 'pre'\n",
    "\n",
    "i = 0\n",
    "preddy = [\n",
    "    'CRWD_nn','APPN_nn', 'INSP_nn', 'SNY_nn',\n",
    "          'EVBG_nn_'\n",
    "          ,'KMI_nn', \n",
    "          'Amazon_nn'\n",
    "    , 'Exxon_df'\n",
    "]\n",
    "Krusty = [\n",
    "    'CRWD_nn_scored','APPN_nn_scored', 'INSP_nn_scored', 'SNY_nn_scored',\n",
    "          'EVBG_nn_scored',\n",
    "          'KMI_nn_scored', \n",
    "         'Amazon_nn_scored'\n",
    "    , 'Exxon_nn_scored'\n",
    "]\n",
    "Krabby[6] = dataframes[i]\n",
    "Krabby[6].columns = ['number','created_at','full_text', 'retweet_count', 'user_id', \n",
    "                     'user.favourites_count', 'user.followers_count','Name','Symbol', 'dupe']\n",
    "Krabby[6] = Krabby[6].dropna()\n",
    "##    print(\"\\nThe head of Krabby \", i, \"is \\n\", Krabby[i].head())\n",
    "zex = Krabby[6]['full_text'].astype(str)\n",
    "x = zex.to_list()\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(x)\n",
    "num_tokens = [len(tokens) for tokens in x]\n",
    "num_tokens = np.array(num_tokens)\n",
    "max_tokens = np.mean(num_tokens) + 3 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "    ##print(\"\\nThe max number of tokens on\", i, \"is\", max_tokens)\n",
    "x_tokens = tokenizer.texts_to_sequences(x)\n",
    "x_tokens_pad = pad_sequences(x_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "   ## print(\"\\nThe third token is \", x_tokens_pad[2])\n",
    "trippy = loaded_model.predict(x_tokens_pad)\n",
    "print(preddy[6], \"shape is \",trippy.shape)\n",
    "predictions = pd.DataFrame(trippy, columns  = ['neg', 'neu', 'pos'])\n",
    "##preddy.append(predictions)\n",
    "print(\"\\n and the head of  preddy is \\n\", predictions.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Tokenize and then create prediction\n",
    "for i in range(len(Krabby)):\n",
    "    Krabby[i] = dataframes[i]\n",
    "    Krabby[i].columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count',\n",
    "                         'user.followers_count','Name','Symbol', 'dupe']\n",
    "    Krabby[i] = Krabby[i].dropna()\n",
    "##    print(\"\\nThe head of Krabby \", i, \"is \\n\", Krabby[i].head())\n",
    "    zex = Krabby[i]['full_text'].astype(str)\n",
    "    x = zex.to_list()\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    num_tokens = [len(tokens) for tokens in x]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    max_tokens = np.mean(num_tokens) + 3 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    ##print(\"\\nThe max number of tokens on\", i, \"is\", max_tokens)\n",
    "    x_tokens = tokenizer.texts_to_sequences(x)\n",
    "    x_tokens_pad = pad_sequences(x_tokens, maxlen=max_tokens,\n",
    "                            padding=pad, truncating=pad)\n",
    "   ## print(\"\\nThe third token is \", x_tokens_pad[2])\n",
    "    trippy = loaded_model.predict(x_tokens_pad)\n",
    "    print(preddy[i], \"shape is \",trippy.shape)\n",
    "    predictions = pd.DataFrame(trippy, columns  = ['neg', 'neu', 'pos'])\n",
    "    preddy.append(predictions)\n",
    "##    print(\"\\n and the head of  preddy is \\n\", predictions.head())\n",
    "    Krusty[i] = Krabby[i].join(predictions)\n",
    "    Krusty[i].columns = ['row','number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', 'user.followers_count','dupe','Symbol', 'Name', 'neg', 'neu', 'pos']\n",
    "##    print(\"\\nThe head of the \", i, \" dataframe is \\n\", Krusty[i].head())\n",
    "    \n",
    "Krusty[0].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_nn_scored.csv', header = True)\n",
    "Krusty[1].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[2].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[3].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[4].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[5].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[4].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_nn_scored.csv', header = True)\n",
    "\n",
    "Krusty[5].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Exxon_nn_scored.csv', header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for the tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now some math for curiosity's sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print(\"\\n the mean number of tokens per text is \", np.mean(num_tokens))\n",
    "print(\"\\nthe maximum number of tokens in a sequence is\\n\", np.max(num_tokens))\n",
    "\n",
    "## We will only allow 2 standard deviations above/below\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "print(\"\\nthat means the max used is \", max_tokens)\n",
    "\n",
    "## Now we will pre pad the shorter ones\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nthe shape of the padded list is \", x_tokens_pad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we go from tokens back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "##\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"\\n This finished at \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n amazon shape is \", Krusty[4].shape)\n",
    "print(\"\\n amazon head is \", Krusty[4].head())\n",
    "print(\"\\n amazon tail is \", Krusty[4].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
