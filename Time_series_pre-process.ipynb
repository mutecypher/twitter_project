{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e0876c3",
   "metadata": {},
   "source": [
    "## After sentiment analysis and duplication removal\n",
    "### Cut down to the day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63d91956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "461c2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filez_first = [\n",
    "    '/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_sent_no_dupes_first.csv', \n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/AMZN_sent_no_dupes_first.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/Exxon_sent_no_dupes_first.csv'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "filez_last = [\n",
    "    '/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_sent_no_dupes_last.csv', \n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/AMZN_sent_no_dupes_last.csv',\n",
    "'/Volumes/Elements/GitHub/twitter-project/Data Files/Exxon_sent_no_dupes_last.csv']\n",
    "\n",
    "\n",
    "dataframes_first = []\n",
    "dataframes_last= []\n",
    "\n",
    "col_names = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', \n",
    "                     'user.followers_count','Symbol', 'Name', 'dupe','neg', 'neu', 'pos']\n",
    "i = 0\n",
    "for filename in filez_first:\n",
    "    dataframes_first.append(pd.read_csv(filename ,header=0, names = col_names, index_col=0, parse_dates=True))\n",
    "\n",
    "i = 0\n",
    "for filename in filez_last:\n",
    "    dataframes_last.append(pd.read_csv(filename, header=0, names = col_names, index_col=0, parse_dates=True))\n",
    "\n",
    "    \n",
    "Original_data_1 = ['CRWD_df', 'APPN_df',  \n",
    "          'GMED_df',\n",
    "          'INSP_df', 'SNY_df','EVBG_df','KMI_df',\n",
    "                 'Amzn_df', \n",
    "                   'XOM_df'\n",
    "                  ]    \n",
    "Original_data_2 = ['CRWD_df', 'APPN_df',  \n",
    "          'GMED_df',\n",
    "          'INSP_df', 'SNY_df','EVBG_df','KMI_df',\n",
    "                'Amzn_df', \n",
    "                   'XOM_df'\n",
    "                  ]\n",
    "Namez = ['_CRWD_df', '_APPN_df',  \n",
    "          'GMED_df',\n",
    "          '_INSP_df', '_SNY_df','_EVBG_df','_KMI_df',\n",
    "         '_Amzn_df', \n",
    "         '_XOM_df'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63ba267e-a652-47a0-8ca5-1d5fa05041b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon last shape  (24441, 13)\n",
      "amazon first shape  (24441, 13)\n",
      "       number     created_at  \\\n",
      "56409    2845   8/3/20 17:43   \n",
      "56410    1461  8/11/21 18:35   \n",
      "56418    9073   7/2/20 14:37   \n",
      "56420    1653  7/29/20 20:02   \n",
      "\n",
      "                                               full_text  retweet_count  \\\n",
      "56409  Big tech returns in 2008 recession...\\n$MSFT (...            0.0   \n",
      "56410  RT @PDX_Options: $AMZN 4HR fell below 3300 and...            2.0   \n",
      "56418                          $AMZN to touch $3K by EOW            0.0   \n",
      "56420  RT @RichLightShed: we expect to eventually rea...            2.0   \n",
      "\n",
      "            user_id  user.favourites_count  user.followers_count  Symbol  \\\n",
      "56409  6.843846e+07                  759.0                 241.0     1.0   \n",
      "56410  1.420000e+18                  280.0                   5.0     1.0   \n",
      "56418  2.279884e+09                 3157.0                 814.0     1.0   \n",
      "56420  1.522249e+08                36833.0                2702.0     1.0   \n",
      "\n",
      "       Name   dupe       neg       neu       pos  \n",
      "56409   1.0  False  0.030252  0.015192  0.954556  \n",
      "56410   0.0  False  0.001445  0.015192  0.983363  \n",
      "56418   0.0  False  0.001422  0.015197  0.983381  \n",
      "56420   0.0  False  0.003937  0.015200  0.980863  \n",
      "    number      created_at                                          full_text  \\\n",
      "35    5947   8/11/20 17:26  @MissionWinners Sent you email on AMZN video. ...   \n",
      "36    1382   7/22/21 20:17  $FB split is coming!!!!!!!!!!! Calling it!\\n$S...   \n",
      "39    1923   7/12/21 20:31  CRASHING THROUGH THE SNOW 2021 1080P AMZN WEBR...   \n",
      "47    2633  10/13/20 19:25  RT @BrianFeroldi: $SQ is 11 years old &amp; wo...   \n",
      "55    1539   4/26/21 14:55  #Options Flow Stream Update $AMZN $TSLA $DIS $...   \n",
      "\n",
      "    retweet_count       user_id  user.favourites_count  user.followers_count  \\\n",
      "35            0.0  7.687251e+08                   56.0                   4.0   \n",
      "36            0.0  1.220000e+18                  407.0                  18.0   \n",
      "39            0.0  4.888262e+09                 1962.0                4072.0   \n",
      "47           18.0  1.150000e+18                 1313.0                  33.0   \n",
      "55            0.0  7.070000e+17                    0.0                3879.0   \n",
      "\n",
      "    Symbol  Name   dupe       neg       neu       pos  \n",
      "35     1.0   0.0  False  0.000578  0.001473  0.997949  \n",
      "36     1.0   0.0  False  0.000841  0.001476  0.997683  \n",
      "39     1.0   0.0  False  0.000741  0.001507  0.997753  \n",
      "47     1.0   0.0   True  0.000863  0.001507  0.997629  \n",
      "55     1.0   0.0  False  0.000814  0.001515  0.997672  \n",
      "amazon last shape  (24441, 13)\n",
      "amazon first shape  (24441, 13)\n",
      "56409      8/3/20 17:43\n",
      "56410     8/11/21 18:35\n",
      "56418      7/2/20 14:37\n",
      "56420     7/29/20 20:02\n",
      "56427    12/14/20 17:35\n",
      "56431      7/23/21 1:20\n",
      "56438    11/25/20 16:25\n",
      "56440    11/25/20 16:05\n",
      "56443    11/25/20 16:23\n",
      "56445     9/13/20 19:21\n",
      "56465     9/18/20 16:22\n",
      "56469     6/21/21 19:59\n",
      "56473      7/8/20 15:05\n",
      "56476      7/8/20 15:11\n",
      "56494    11/23/20 19:39\n",
      "56519     8/11/21 18:31\n",
      "56531     6/25/20 13:30\n",
      "56546     1/14/21 22:58\n",
      "56555      8/8/21 18:07\n",
      "56575     8/11/21 17:11\n",
      "Name: created_at, dtype: object\n",
      "first went\n"
     ]
    }
   ],
   "source": [
    "print(\"amazon last shape \",dataframes_last[7].shape)\n",
    "print(\"amazon first shape \", dataframes_first[7].shape)\n",
    "print(dataframes_last[7][6520:6524])\n",
    "dately =  pd.to_datetime(dataframes_last[7]['created_at']\n",
    "                         [6532:24467\n",
    "                        ,], utc = True)\n",
    "\n",
    "\n",
    "\n",
    "amzn_first_drops = [6463, 6464, 6465, 6467, 6522, 6525, 6565, 6566, 6567, 6528, 6531]\n",
    "\n",
    "amzn_last_drops = [6522, 6525, 6526, 6527, 6528, 6531]\n",
    "\n",
    "print(dataframes_first[7].head())\n",
    "##dataframes_first[7]['val']  = dataframes_last[7][dataframes_last[7][\"number\" != 7]]\n",
    "\n",
    "print(\"amazon last shape \",dataframes_last[7].shape)\n",
    "print(\"amazon first shape \", dataframes_first[7].shape)\n",
    "\n",
    "print(dataframes_last[7]['created_at'][6520:6540,])\n",
    "\n",
    "##pd.to_datetime(dataframes_first[7]['created_at'], utc = True)\n",
    "\n",
    "print(\"first went\")\n",
    "##pd.to_datetime(dataframes_last[7]['created_at'], utc = True)\n",
    "\n",
    "##dately =  pd.to_datetime(df1['created_at'], utc = True)\n",
    "##print(type(dately))\n",
    "##dately = dately.sort_values()\n",
    "##print(dately.head())                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcc61fb4-b929-491e-af38-5adc70eb48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of  _CRWD_df  (Kept_last) is  (5041, 14)\n",
      "\n",
      " The number of unique dates in  _CRWD_df  (Kept_last) is  328\n",
      "\n",
      "The shape of  _APPN_df  (Kept_last) is  (1, 14)\n",
      "\n",
      " The number of unique dates in  _APPN_df  (Kept_last) is  1\n",
      "\n",
      "The shape of  GMED_df  (Kept_last) is  (793, 14)\n",
      "\n",
      " The number of unique dates in  GMED_df  (Kept_last) is  193\n",
      "\n",
      "The shape of  _INSP_df  (Kept_last) is  (442, 14)\n",
      "\n",
      " The number of unique dates in  _INSP_df  (Kept_last) is  22\n",
      "\n",
      "The shape of  _SNY_df  (Kept_last) is  (12, 14)\n",
      "\n",
      " The number of unique dates in  _SNY_df  (Kept_last) is  9\n",
      "\n",
      "The shape of  _EVBG_df  (Kept_last) is  (2461, 14)\n",
      "\n",
      " The number of unique dates in  _EVBG_df  (Kept_last) is  378\n",
      "\n",
      "The shape of  _KMI_df  (Kept_last) is  (1185, 14)\n",
      "\n",
      " The number of unique dates in  _KMI_df  (Kept_last) is  85\n",
      "\n",
      "The shape of  _Amzn_df  (Kept_last) is  (24441, 14)\n",
      "\n",
      " The number of unique dates in  _Amzn_df  (Kept_last) is  233\n",
      "\n",
      "The shape of  _XOM_df  (Kept_last) is  (2979, 14)\n",
      "\n",
      " The number of unique dates in  _XOM_df  (Kept_last) is  160\n",
      "\n",
      "the shape of kept_first i is  0  is  (5041, 13)\n",
      "\n",
      "The shape of  _CRWD_df  (Kept_first) is  (5041, 14)\n",
      "\n",
      " The number of unique dates in  _CRWD_df  (Kept_first) is  328\n",
      "\n",
      "the shape of kept_first i is  1  is  (1, 13)\n",
      "\n",
      "The shape of  _APPN_df  (Kept_first) is  (1, 14)\n",
      "\n",
      " The number of unique dates in  _APPN_df  (Kept_first) is  1\n",
      "\n",
      "the shape of kept_first i is  2  is  (793, 13)\n",
      "\n",
      "The shape of  GMED_df  (Kept_first) is  (793, 14)\n",
      "\n",
      " The number of unique dates in  GMED_df  (Kept_first) is  193\n",
      "\n",
      "the shape of kept_first i is  3  is  (442, 13)\n",
      "\n",
      "The shape of  _INSP_df  (Kept_first) is  (442, 14)\n",
      "\n",
      " The number of unique dates in  _INSP_df  (Kept_first) is  22\n",
      "\n",
      "the shape of kept_first i is  4  is  (12, 13)\n",
      "\n",
      "The shape of  _SNY_df  (Kept_first) is  (12, 14)\n",
      "\n",
      " The number of unique dates in  _SNY_df  (Kept_first) is  9\n",
      "\n",
      "the shape of kept_first i is  5  is  (2461, 13)\n",
      "\n",
      "The shape of  _EVBG_df  (Kept_first) is  (2461, 14)\n",
      "\n",
      " The number of unique dates in  _EVBG_df  (Kept_first) is  378\n",
      "\n",
      "the shape of kept_first i is  6  is  (1185, 13)\n",
      "\n",
      "The shape of  _KMI_df  (Kept_first) is  (1185, 14)\n",
      "\n",
      " The number of unique dates in  _KMI_df  (Kept_first) is  85\n",
      "\n",
      "the shape of kept_first i is  7  is  (24441, 13)\n",
      "\n",
      "The shape of  _Amzn_df  (Kept_first) is  (24441, 14)\n",
      "\n",
      " The number of unique dates in  _Amzn_df  (Kept_first) is  233\n",
      "\n",
      "the shape of kept_first i is  8  is  (2979, 13)\n",
      "\n",
      "The shape of  _XOM_df  (Kept_first) is  (2979, 14)\n",
      "\n",
      " The number of unique dates in  _XOM_df  (Kept_first) is  160\n"
     ]
    }
   ],
   "source": [
    "##dataframes_first[0]['date'] = dataframes_first[0]['created_at'].dt.normalize()\n",
    "\n",
    "##i=0\n",
    "Kept_last = Original_data_1 \n",
    "Kept_first = Original_data_2\n",
    "\n",
    "\n",
    "for i in range(len(Kept_last)):\n",
    "    Kept_last[i] = dataframes_last[i]\n",
    "    Kept_last[i].columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', \n",
    "                     'user.followers_count','Symbol', 'Name', 'dupe','neg', 'neu', 'pos']\n",
    "    \n",
    "    Kept_last[i]['date'] = pd.to_datetime(Kept_last[i]['created_at'], utc = True)\n",
    "    Kept_last[i]['date']= Kept_last[i]['date'].dt.date\n",
    "    Kept_last[i] = Kept_last[i].sort_values(by = 'date')\n",
    "    print(\"\\nThe shape of \", Namez[i],\" (Kept_last) is \",  Kept_last[i].shape)\n",
    "    print(\"\\n The number of unique dates in \",Namez[i], \" (Kept_last) is \", Kept_last[i]['date'].nunique())\n",
    "    \n",
    "##i = 0\n",
    "    \n",
    "for i in range(len(Kept_first)):\n",
    "    Kept_first[i] = dataframes_first[i]\n",
    "    Kept_first[i].columns = ['number','created_at','full_text', 'retweet_count', 'user_id', 'user.favourites_count', \n",
    "                     'user.followers_count','Symbol', 'Name','dupe', 'neg', 'neu', 'pos']\n",
    "    print(\"\\nthe shape of kept_first i is \", i,\" is \", Kept_first[i].shape)\n",
    "    Kept_first[i]['date'] = pd.to_datetime(Kept_first[i]['created_at'], utc = True)\n",
    "    Kept_first[i]['date']= Kept_first[i]['date'].dt.date\n",
    "    Kept_first[i] = Kept_first[i].sort_values(by = 'date')\n",
    "    print(\"\\nThe shape of \", Namez[i],\" (Kept_first) is \",  Kept_first[i].shape)\n",
    "    print(\"\\n The number of unique dates in \",Namez[i], \" (Kept_first) is \", Kept_first[i]['date'].nunique())\n",
    "##    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56849c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fcffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1c91d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior.  In a future version these will be considered non-comparable.Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The shape of Kept_last  _CRWD_df  is  (328, 14)\n",
      "\n",
      " The shape of Kept_last  _APPN_df  is  (1, 14)\n",
      "\n",
      " The shape of Kept_last  GMED_df  is  (193, 14)\n",
      "\n",
      " The shape of Kept_last  _INSP_df  is  (22, 14)\n",
      "\n",
      " The shape of Kept_last  _SNY_df  is  (9, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:29: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: All-NaN axis encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The shape of Kept_last  _EVBG_df  is  (378, 14)\n",
      "\n",
      " The shape of Kept_last  _KMI_df  is  (85, 14)\n",
      "\n",
      " The shape of Kept_last  _Amzn_df  is  (233, 14)\n",
      "\n",
      " The shape of Kept_last  _XOM_df  is  (160, 14)\n"
     ]
    }
   ],
   "source": [
    "Kept_last_analysis = Kept_last\n",
    "for k in range(len(Kept_last_analysis)):\n",
    "    \n",
    "    amznz = Kept_last_analysis[k]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = 0\n",
    "    avg_amzn = pd.DataFrame()\n",
    "    bob = pd.to_datetime(np.unique(Kept_last_analysis[k]['date']))\n",
    "\n",
    "    for j in range(len(bob)):\n",
    "        mask = (Kept_last_analysis[k][\"date\"] ==  bob[j])\n",
    "        squab = pd.DataFrame(Kept_last_analysis[k][mask])\n",
    "        squab = squab.reset_index(drop = True)\n",
    "##        print(\"\\n Squab number of rows is \", j,\"  \", squab.shape[0])\n",
    "##        print(\"\\n All the dates are \", squab.loc[0,'date'])\n",
    "        depth = squab.shape[0]\n",
    "##        print(\"\\n The date under discussion is \", squab.loc[0,'date'])\n",
    "        text_date = squab.loc[0,'date']\n",
    "        avg_amzn.loc[j,'date'] = text_date\n",
    "        avg_amzn.loc[j, 'flat_avg_neg'] = sum(squab['neg'])/ depth\n",
    "        avg_amzn.loc[j, 'flat_avg_neu'] = sum(squab['neu'])/ depth\n",
    "        avg_amzn.loc[j, 'flat_avg_pos'] = sum(squab['pos'])/ depth\n",
    "        avg_amzn.loc[j, 'number_of_same_dates'] = depth\n",
    "        avg_amzn.loc[j,'weighted_avg_neg'] = sum((squab.loc[0:depth,'neg']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'weighted_avg_neu'] = sum((squab.loc[0:depth,'neu']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'weighted_avg_pos'] = sum((squab.loc[0:depth,'pos']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'min_neg'] = np.nanmin(squab['neg'])\n",
    "        avg_amzn.loc[j,'min_neu'] = np.nanmin(squab['neu'])\n",
    "        avg_amzn.loc[j,'min_pos'] = np.nanmin(squab['pos'])\n",
    "        avg_amzn.loc[j,'max_neg'] = np.nanmax(squab['neg'])\n",
    "        avg_amzn.loc[j,'max_neu'] = np.nanmax(squab['neu'])\n",
    "        avg_amzn.loc[j,'max_pos'] = np.nanmax(squab['pos'])\n",
    "        \n",
    "    Kept_last_analysis[k] = avg_amzn\n",
    "    print(\"\\n The shape of Kept_last \", Namez[k],\" is \", avg_amzn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "041bf942",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kept_last_analysis[0].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[1].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[2].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[3].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[4].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[5].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[6].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[7].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_last_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_last_analysis[8].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Exxon_last_kept_preproc.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c6b1689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The shape of Kept_first  _CRWD_df  is  (328, 14)\n",
      "\n",
      " The shape of Kept_first  _APPN_df  is  (1, 14)\n",
      "\n",
      " The shape of Kept_first  GMED_df  is  (193, 14)\n",
      "\n",
      " The shape of Kept_first  _INSP_df  is  (22, 14)\n",
      "\n",
      " The shape of Kept_first  _SNY_df  is  (9, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:30: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:33: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:34: RuntimeWarning: All-NaN axis encountered\n",
      "/Users/mutecypher/opt/anaconda3/envs/for_tensorflow/lib/python3.7/site-packages/ipykernel_launcher.py:35: RuntimeWarning: All-NaN axis encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The shape of Kept_first  _EVBG_df  is  (378, 14)\n",
      "\n",
      " The shape of Kept_first  _KMI_df  is  (85, 14)\n",
      "\n",
      " The shape of Kept_first  _Amzn_df  is  (233, 14)\n",
      "\n",
      " The shape of Kept_first  _XOM_df  is  (160, 14)\n"
     ]
    }
   ],
   "source": [
    "Kept_first_analysis = Kept_first\n",
    "\n",
    "\n",
    "for k in range(len(Kept_first_analysis)):\n",
    "    \n",
    "    amznz = Kept_first_analysis[k]\n",
    "    i = 0\n",
    "    j = 0\n",
    "    n = 0\n",
    "    avg_amzn = pd.DataFrame()\n",
    "    bob = pd.to_datetime(np.unique(Kept_first_analysis[k]['date']))\n",
    "\n",
    "    for j in range(len(bob)):\n",
    "        mask = (Kept_first_analysis[k][\"date\"] ==  bob[j])\n",
    "        squab = pd.DataFrame(Kept_first_analysis[k][mask])\n",
    "        squab = squab.reset_index(drop = True)\n",
    "##        print(\"\\n Squab number of rows is \", j,\"  \", squab.shape[0])\n",
    "##        print(\"\\n All the dates are \", squab.loc[0,'date'])\n",
    "        depth = squab.shape[0]\n",
    "##        print(\"\\n The date under discussion is \", squab.loc[0,'date'])\n",
    "        text_date = squab.loc[0,'date']\n",
    "        avg_amzn.loc[j,'date'] = text_date\n",
    "        avg_amzn.loc[j, 'flat_avg_neg'] = sum(squab['neg'])/ depth\n",
    "        avg_amzn.loc[j, 'flat_avg_neu'] = sum(squab['neu'])/ depth\n",
    "        avg_amzn.loc[j, 'flat_avg_pos'] = sum(squab['pos'])/ depth\n",
    "        avg_amzn.loc[j, 'number_of_same_dates'] = depth\n",
    "        avg_amzn.loc[j,'weighted_avg_neg'] = sum((squab.loc[0:depth,'neg']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'weighted_avg_neu'] = sum((squab.loc[0:depth,'neu']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'weighted_avg_pos'] = sum((squab.loc[0:depth,'pos']*squab.loc[0:depth,'user.favourites_count']))/(sum(squab['user.favourites_count'])+1)\n",
    "        avg_amzn.loc[j,'min_neg'] = np.nanmin(squab['neg'])\n",
    "        avg_amzn.loc[j,'min_neu'] = np.nanmin(squab['neu'])\n",
    "        avg_amzn.loc[j,'min_pos'] = np.nanmin(squab['pos'])\n",
    "        avg_amzn.loc[j,'max_neg'] = np.nanmax(squab['neg'])\n",
    "        avg_amzn.loc[j,'max_neu'] = np.nanmax(squab['neu'])\n",
    "        avg_amzn.loc[j,'max_pos'] = np.nanmax(squab['pos'])\n",
    "        \n",
    "    Kept_first_analysis[k] = avg_amzn\n",
    "    print(\"\\n The shape of Kept_first \", Namez[k],\" is \", avg_amzn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63678a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kept_first_analysis[0].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/CRWD_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[1].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/APPN_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[2].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/GMED_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[3].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/INSP_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[4].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/SNY_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[5].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/EVBG_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[6].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/KMI_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[7].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Amazon_first_kept_preproc.csv', header = True)\n",
    "\n",
    "Kept_first_analysis[8].to_csv('/Volumes/Elements/GitHub/twitter-project/Data Files/Exxon_first_kept_preproc.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7724dcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This finished at  2022-02-19 18:18:16.446595\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = dt.datetime.now()\n",
    "print(\"\\n This finished at \", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c7bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6897080",
   "metadata": {},
   "source": [
    "## Combine daily data into one number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d47bfa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "unique values in amzn date are\n",
      " 0    2021-10-29\n",
      "1    2021-10-31\n",
      "2    2021-11-01\n",
      "3    2021-11-02\n",
      "Name: date, dtype: object\n",
      "2021-10-31\n",
      "\n",
      " This is  (1, 14)\n"
     ]
    }
   ],
   "source": [
    "##print(\"unique values in amzn date are \", np.unique(Kept_last_analysis[6][\"date\"]))\n",
    "print(\"\\nunique values in amzn date are\\n\", Kept_last_analysis[6][\"date\"][0:4,])\n",
    "task = Kept_last_analysis[6].loc[1,\"date\"]\n",
    "print(task)\n",
    "mask = (Kept_last_analysis[6][\"date\"] ==  task)\n",
    "bob =  pd.DataFrame(Kept_last_analysis[6][mask])\n",
    "print(\"\\n This is \", bob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc83ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63b35ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon first few is \n",
      "          date  flat_avg_neg  flat_avg_neu  flat_avg_pos  number_of_same_dates  \\\n",
      "0  2020-06-24      0.081976      0.454402      0.463622                 314.0   \n",
      "1  2020-06-25      0.076232      0.540759      0.383009                 856.0   \n",
      "2  2020-06-26      0.092678      0.559408      0.347915                 107.0   \n",
      "3  2020-06-27      0.061732      0.715758      0.222509                  20.0   \n",
      "4  2020-06-28      0.101550      0.460008      0.438442                  23.0   \n",
      "\n",
      "   weighted_avg_neg  weighted_avg_neu  weighted_avg_pos   min_neg   min_neu  \\\n",
      "0          0.102342          0.410040          0.487618  0.000464  0.001805   \n",
      "1          0.112850          0.520682          0.366468  0.000441  0.001772   \n",
      "2          0.126983          0.475099          0.397917  0.000726  0.002264   \n",
      "3          0.022118          0.679987          0.297890  0.001158  0.005275   \n",
      "4          0.080943          0.545341          0.373712  0.000965  0.004019   \n",
      "\n",
      "    min_pos   max_neg   max_neu   max_pos  \n",
      "0  0.001615  0.981268  0.990884  0.997731  \n",
      "1  0.001856  0.990294  0.991422  0.997722  \n",
      "2  0.001828  0.988336  0.991145  0.997010  \n",
      "3  0.007420  0.635762  0.989122  0.993567  \n",
      "4  0.006792  0.964538  0.990157  0.995016  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAmazon first few is \\n\", Kept_first_analysis[7].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a907e4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " This finished at  2022-02-19 18:18:16.469176\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"\\n This finished at \", now)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
